{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68793758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 8.0639e-01, PDE Loss: 1.7312e-04, BC Loss: 5.6298e-02, IC Loss: 6.1033e-01, Data Loss: 1.3959e-01\n",
      "Iter: 200, Loss: 9.1623e-03, PDE Loss: 5.8478e-03, BC Loss: 1.2202e-03, IC Loss: 2.0941e-03, Data Loss: 2.2584e-07\n",
      "Iter: 400, Loss: 4.9645e-03, PDE Loss: 3.8082e-03, BC Loss: 8.6252e-04, IC Loss: 2.9330e-04, Data Loss: 5.0530e-07\n",
      "Iter: 600, Loss: 4.1456e-03, PDE Loss: 2.9885e-03, BC Loss: 1.0001e-03, IC Loss: 1.5690e-04, Data Loss: 9.8510e-08\n",
      "Iter: 800, Loss: 3.6888e-03, PDE Loss: 2.4638e-03, BC Loss: 1.0829e-03, IC Loss: 1.4169e-04, Data Loss: 4.3185e-07\n",
      "Iter: 1000, Loss: 3.3800e-03, PDE Loss: 2.1177e-03, BC Loss: 1.1223e-03, IC Loss: 1.3938e-04, Data Loss: 6.0124e-07\n",
      "Iter: 1200, Loss: 5.0285e-03, PDE Loss: 1.7787e-03, BC Loss: 1.1584e-03, IC Loss: 9.9036e-04, Data Loss: 1.1011e-03\n",
      "Iter: 1400, Loss: 2.9169e-03, PDE Loss: 1.6718e-03, BC Loss: 1.1329e-03, IC Loss: 1.1108e-04, Data Loss: 1.1542e-06\n",
      "Iter: 1600, Loss: 3.5296e-03, PDE Loss: 1.5360e-03, BC Loss: 1.0980e-03, IC Loss: 4.3619e-04, Data Loss: 4.5939e-04\n",
      "Iter: 1800, Loss: 2.6354e-03, PDE Loss: 1.4440e-03, BC Loss: 1.1072e-03, IC Loss: 8.3365e-05, Data Loss: 9.2869e-07\n",
      "Iter: 2000, Loss: 2.5408e-03, PDE Loss: 1.3764e-03, BC Loss: 1.0884e-03, IC Loss: 7.3313e-05, Data Loss: 2.5954e-06\n",
      "Iter: 2200, Loss: 2.6921e-03, PDE Loss: 1.3461e-03, BC Loss: 1.0986e-03, IC Loss: 1.6077e-04, Data Loss: 8.6688e-05\n",
      "Iter: 2400, Loss: 2.3963e-03, PDE Loss: 1.2709e-03, BC Loss: 1.0676e-03, IC Loss: 5.7245e-05, Data Loss: 6.0228e-07\n",
      "Iter: 2600, Loss: 2.3421e-03, PDE Loss: 1.2412e-03, BC Loss: 1.0513e-03, IC Loss: 4.9389e-05, Data Loss: 1.8401e-07\n",
      "Iter: 2800, Loss: 2.3372e-03, PDE Loss: 1.2212e-03, BC Loss: 1.0343e-03, IC Loss: 7.6256e-05, Data Loss: 5.4516e-06\n",
      "Iter: 3000, Loss: 2.2342e-03, PDE Loss: 1.1797e-03, BC Loss: 1.0136e-03, IC Loss: 4.0604e-05, Data Loss: 2.9065e-07\n",
      "Iter: 3200, Loss: 2.1946e-03, PDE Loss: 1.1624e-03, BC Loss: 9.9478e-04, IC Loss: 3.7151e-05, Data Loss: 2.3209e-07\n",
      "Iter: 3400, Loss: 2.1592e-03, PDE Loss: 1.1498e-03, BC Loss: 9.7445e-04, IC Loss: 3.4703e-05, Data Loss: 1.8431e-07\n",
      "Iter: 3600, Loss: 2.2002e-03, PDE Loss: 1.1653e-03, BC Loss: 9.3733e-04, IC Loss: 8.2209e-05, Data Loss: 1.5361e-05\n",
      "Iter: 3800, Loss: 2.2494e-03, PDE Loss: 1.1233e-03, BC Loss: 9.2830e-04, IC Loss: 1.1384e-04, Data Loss: 8.3956e-05\n",
      "Iter: 4000, Loss: 2.0532e-03, PDE Loss: 1.1117e-03, BC Loss: 9.1267e-04, IC Loss: 2.8708e-05, Data Loss: 6.3583e-08\n",
      "Iter: 4200, Loss: 2.0294e-03, PDE Loss: 1.1018e-03, BC Loss: 8.9903e-04, IC Loss: 2.8488e-05, Data Loss: 8.6690e-08\n",
      "Iter: 4400, Loss: 2.0094e-03, PDE Loss: 1.0970e-03, BC Loss: 8.8008e-04, IC Loss: 3.0166e-05, Data Loss: 2.1591e-06\n",
      "Iter: 4600, Loss: 1.9614e-03, PDE Loss: 1.0713e-03, BC Loss: 8.6315e-04, IC Loss: 2.6836e-05, Data Loss: 5.5333e-08\n",
      "Iter: 4800, Loss: 1.9353e-03, PDE Loss: 1.0606e-03, BC Loss: 8.4708e-04, IC Loss: 2.7522e-05, Data Loss: 9.2696e-08\n",
      "Iter: 5000, Loss: 1.9074e-03, PDE Loss: 1.0473e-03, BC Loss: 8.3288e-04, IC Loss: 2.6926e-05, Data Loss: 2.7695e-07\n",
      "Iter: 5200, Loss: 5.2542e-03, PDE Loss: 1.1047e-03, BC Loss: 2.8156e-03, IC Loss: 7.2229e-04, Data Loss: 6.1162e-04\n",
      "Iter: 5400, Loss: 1.8453e-03, PDE Loss: 1.0169e-03, BC Loss: 8.0221e-04, IC Loss: 2.6121e-05, Data Loss: 3.9095e-08\n",
      "Iter: 5600, Loss: 1.8143e-03, PDE Loss: 1.0018e-03, BC Loss: 7.8698e-04, IC Loss: 2.5421e-05, Data Loss: 1.1304e-07\n",
      "Iter: 5800, Loss: 1.7890e-03, PDE Loss: 9.9000e-04, BC Loss: 7.7197e-04, IC Loss: 2.6775e-05, Data Loss: 2.6929e-07\n",
      "Iter: 6000, Loss: 1.7595e-03, PDE Loss: 9.7852e-04, BC Loss: 7.5387e-04, IC Loss: 2.6365e-05, Data Loss: 7.1846e-07\n",
      "Iter: 6200, Loss: 1.7218e-03, PDE Loss: 9.5824e-04, BC Loss: 7.3983e-04, IC Loss: 2.3573e-05, Data Loss: 1.4597e-07\n",
      "Iter: 6400, Loss: 1.7359e-03, PDE Loss: 9.4448e-04, BC Loss: 7.3433e-04, IC Loss: 3.6000e-05, Data Loss: 2.1104e-05\n",
      "Iter: 6600, Loss: 1.6590e-03, PDE Loss: 9.2850e-04, BC Loss: 7.0794e-04, IC Loss: 2.2474e-05, Data Loss: 1.2432e-07\n",
      "Iter: 6800, Loss: 1.6240e-03, PDE Loss: 9.1051e-04, BC Loss: 6.9154e-04, IC Loss: 2.1870e-05, Data Loss: 7.1448e-08\n",
      "Iter: 7000, Loss: 1.6055e-03, PDE Loss: 8.8716e-04, BC Loss: 6.7223e-04, IC Loss: 3.8463e-05, Data Loss: 7.6024e-06\n",
      "Iter: 7200, Loss: 2.1707e-03, PDE Loss: 8.8228e-04, BC Loss: 9.6804e-04, IC Loss: 1.5730e-04, Data Loss: 1.6305e-04\n",
      "Iter: 7400, Loss: 1.4695e-03, PDE Loss: 8.2281e-04, BC Loss: 6.2695e-04, IC Loss: 1.9587e-05, Data Loss: 1.1896e-07\n",
      "Iter: 7600, Loss: 1.4236e-03, PDE Loss: 7.9769e-04, BC Loss: 6.0695e-04, IC Loss: 1.8850e-05, Data Loss: 1.1317e-07\n",
      "Iter: 7800, Loss: 1.3817e-03, PDE Loss: 7.6929e-04, BC Loss: 5.9428e-04, IC Loss: 1.8032e-05, Data Loss: 1.2189e-07\n",
      "Iter: 8000, Loss: 1.3543e-03, PDE Loss: 7.5005e-04, BC Loss: 5.8678e-04, IC Loss: 1.7115e-05, Data Loss: 3.3161e-07\n",
      "Iter: 8200, Loss: 1.3309e-03, PDE Loss: 7.2898e-04, BC Loss: 5.8706e-04, IC Loss: 1.4788e-05, Data Loss: 3.3027e-08\n",
      "Iter: 8400, Loss: 1.3119e-03, PDE Loss: 7.1421e-04, BC Loss: 5.8281e-04, IC Loss: 1.4785e-05, Data Loss: 7.6019e-08\n",
      "Iter: 8600, Loss: 1.3074e-03, PDE Loss: 6.9537e-04, BC Loss: 5.8853e-04, IC Loss: 2.0390e-05, Data Loss: 3.0906e-06\n",
      "Iter: 8800, Loss: 1.2788e-03, PDE Loss: 6.8712e-04, BC Loss: 5.7850e-04, IC Loss: 1.3146e-05, Data Loss: 4.9501e-08\n",
      "Iter: 9000, Loss: 1.2955e-03, PDE Loss: 6.7827e-04, BC Loss: 5.7096e-04, IC Loss: 2.9651e-05, Data Loss: 1.6610e-05\n",
      "Iter: 9200, Loss: 1.2435e-03, PDE Loss: 6.6023e-04, BC Loss: 5.7269e-04, IC Loss: 1.0535e-05, Data Loss: 8.8662e-09\n",
      "Iter: 9400, Loss: 1.2222e-03, PDE Loss: 6.4896e-04, BC Loss: 5.6163e-04, IC Loss: 1.1352e-05, Data Loss: 2.4058e-07\n",
      "Iter: 9600, Loss: 1.1961e-03, PDE Loss: 6.3265e-04, BC Loss: 5.5270e-04, IC Loss: 1.0675e-05, Data Loss: 6.8663e-08\n",
      "Iter: 9800, Loss: 1.2643e-03, PDE Loss: 6.1283e-04, BC Loss: 5.9609e-04, IC Loss: 4.1446e-05, Data Loss: 1.3945e-05\n",
      "Iter: 10000, Loss: 1.1758e-03, PDE Loss: 6.1038e-04, BC Loss: 5.4207e-04, IC Loss: 1.4835e-05, Data Loss: 8.5081e-06\n",
      "Iter: 10200, Loss: 1.1786e-03, PDE Loss: 6.0350e-04, BC Loss: 5.5610e-04, IC Loss: 1.4123e-05, Data Loss: 4.9183e-06\n",
      "Iter: 10400, Loss: 1.1000e-03, PDE Loss: 5.8527e-04, BC Loss: 5.0598e-04, IC Loss: 8.6748e-06, Data Loss: 2.6916e-08\n",
      "Iter: 10600, Loss: 1.6835e-03, PDE Loss: 5.6642e-04, BC Loss: 5.9794e-04, IC Loss: 2.3733e-04, Data Loss: 2.8185e-04\n",
      "Iter: 10800, Loss: 1.0662e-03, PDE Loss: 5.5576e-04, BC Loss: 4.9917e-04, IC Loss: 1.0305e-05, Data Loss: 9.9396e-07\n",
      "Iter: 11000, Loss: 1.0211e-03, PDE Loss: 5.3935e-04, BC Loss: 4.7433e-04, IC Loss: 7.4430e-06, Data Loss: 2.3569e-09\n",
      "Iter: 11200, Loss: 9.9211e-04, PDE Loss: 5.2303e-04, BC Loss: 4.6202e-04, IC Loss: 7.0691e-06, Data Loss: 4.9098e-10\n",
      "Iter: 11400, Loss: 9.6265e-04, PDE Loss: 5.0694e-04, BC Loss: 4.4894e-04, IC Loss: 6.7639e-06, Data Loss: 1.6319e-09\n",
      "Iter: 11600, Loss: 1.2268e-03, PDE Loss: 4.9815e-04, BC Loss: 4.8019e-04, IC Loss: 1.1487e-04, Data Loss: 1.3360e-04\n",
      "Iter: 11800, Loss: 9.9582e-04, PDE Loss: 4.6809e-04, BC Loss: 4.4486e-04, IC Loss: 8.2837e-05, Data Loss: 3.8695e-08\n",
      "Iter: 12000, Loss: 8.8050e-04, PDE Loss: 4.5239e-04, BC Loss: 4.1629e-04, IC Loss: 7.9275e-06, Data Loss: 3.9004e-06\n",
      "Iter: 12200, Loss: 8.4581e-04, PDE Loss: 4.3418e-04, BC Loss: 4.0619e-04, IC Loss: 5.4464e-06, Data Loss: 2.9501e-09\n",
      "Iter: 12400, Loss: 8.2247e-04, PDE Loss: 4.1642e-04, BC Loss: 4.0079e-04, IC Loss: 5.2599e-06, Data Loss: 4.2593e-10\n",
      "Iter: 12600, Loss: 8.0434e-04, PDE Loss: 4.0226e-04, BC Loss: 3.9724e-04, IC Loss: 4.8360e-06, Data Loss: 4.0827e-09\n",
      "Iter: 12800, Loss: 7.8930e-04, PDE Loss: 3.8823e-04, BC Loss: 3.9653e-04, IC Loss: 4.5374e-06, Data Loss: 2.0710e-09\n",
      "Iter: 13000, Loss: 7.7774e-04, PDE Loss: 3.7831e-04, BC Loss: 3.9523e-04, IC Loss: 4.1776e-06, Data Loss: 1.1813e-08\n",
      "Iter: 13200, Loss: 8.2436e-04, PDE Loss: 3.6957e-04, BC Loss: 4.0639e-04, IC Loss: 2.1969e-05, Data Loss: 2.6435e-05\n",
      "Iter: 13400, Loss: 7.5817e-04, PDE Loss: 3.5696e-04, BC Loss: 3.9703e-04, IC Loss: 4.1692e-06, Data Loss: 8.1893e-09\n",
      "Iter: 13600, Loss: 9.4295e-04, PDE Loss: 3.5133e-04, BC Loss: 4.4540e-04, IC Loss: 6.7712e-05, Data Loss: 7.8514e-05\n",
      "Iter: 13800, Loss: 7.4832e-04, PDE Loss: 3.4628e-04, BC Loss: 3.9642e-04, IC Loss: 3.9286e-06, Data Loss: 1.6869e-06\n",
      "Iter: 14000, Loss: 7.4962e-04, PDE Loss: 3.4166e-04, BC Loss: 3.9905e-04, IC Loss: 6.0620e-06, Data Loss: 2.8514e-06\n",
      "Iter: 14200, Loss: 9.8627e-04, PDE Loss: 3.4840e-04, BC Loss: 5.2127e-04, IC Loss: 1.1529e-04, Data Loss: 1.3140e-06\n",
      "Iter: 14400, Loss: 8.2442e-04, PDE Loss: 3.4028e-04, BC Loss: 3.9995e-04, IC Loss: 7.1244e-05, Data Loss: 1.2947e-05\n",
      "Iter: 14600, Loss: 9.4518e-04, PDE Loss: 3.2215e-04, BC Loss: 5.3667e-04, IC Loss: 7.0693e-05, Data Loss: 1.5675e-05\n",
      "Iter: 14800, Loss: 7.2544e-04, PDE Loss: 3.2947e-04, BC Loss: 3.9244e-04, IC Loss: 3.5143e-06, Data Loss: 1.9032e-08\n",
      "Iter: 15000, Loss: 7.3411e-04, PDE Loss: 3.2324e-04, BC Loss: 3.9888e-04, IC Loss: 1.0967e-05, Data Loss: 1.0282e-06\n",
      "Iter: 15200, Loss: 1.2707e-03, PDE Loss: 3.2697e-04, BC Loss: 5.0612e-04, IC Loss: 1.9913e-04, Data Loss: 2.3853e-04\n",
      "Iter: 15400, Loss: 7.5472e-04, PDE Loss: 3.2399e-04, BC Loss: 4.0003e-04, IC Loss: 1.6500e-05, Data Loss: 1.4201e-05\n",
      "Iter: 15600, Loss: 7.6972e-04, PDE Loss: 3.2380e-04, BC Loss: 4.1743e-04, IC Loss: 2.1738e-05, Data Loss: 6.7569e-06\n",
      "Iter: 15800, Loss: 7.1420e-04, PDE Loss: 3.2152e-04, BC Loss: 3.8974e-04, IC Loss: 2.9289e-06, Data Loss: 7.4541e-09\n",
      "Iter: 16000, Loss: 7.1184e-04, PDE Loss: 3.2133e-04, BC Loss: 3.8710e-04, IC Loss: 3.3593e-06, Data Loss: 4.6756e-08\n",
      "Iter: 16200, Loss: 7.1933e-04, PDE Loss: 3.2037e-04, BC Loss: 3.9232e-04, IC Loss: 5.0428e-06, Data Loss: 1.5970e-06\n",
      "Iter: 16400, Loss: 7.2444e-04, PDE Loss: 3.1424e-04, BC Loss: 4.0306e-04, IC Loss: 6.2523e-06, Data Loss: 8.8561e-07\n",
      "Iter: 16600, Loss: 7.0598e-04, PDE Loss: 3.1897e-04, BC Loss: 3.8347e-04, IC Loss: 3.4446e-06, Data Loss: 9.5208e-08\n",
      "Iter: 16800, Loss: 7.1919e-04, PDE Loss: 3.1240e-04, BC Loss: 3.8902e-04, IC Loss: 1.0685e-05, Data Loss: 7.0811e-06\n",
      "Iter: 17000, Loss: 7.3238e-04, PDE Loss: 3.2353e-04, BC Loss: 3.9333e-04, IC Loss: 1.4063e-05, Data Loss: 1.4574e-06\n",
      "Iter: 17200, Loss: 6.9776e-04, PDE Loss: 3.1229e-04, BC Loss: 3.8233e-04, IC Loss: 3.1380e-06, Data Loss: 8.2623e-11\n",
      "Iter: 17400, Loss: 8.4632e-04, PDE Loss: 3.1509e-04, BC Loss: 4.0625e-04, IC Loss: 5.9570e-05, Data Loss: 6.5405e-05\n",
      "Iter: 17600, Loss: 6.9689e-04, PDE Loss: 3.0666e-04, BC Loss: 3.8423e-04, IC Loss: 5.5394e-06, Data Loss: 4.5872e-07\n",
      "Iter: 17800, Loss: 1.0487e-03, PDE Loss: 3.1219e-04, BC Loss: 4.6391e-04, IC Loss: 1.3272e-04, Data Loss: 1.3987e-04\n",
      "Iter: 18000, Loss: 8.4295e-04, PDE Loss: 3.3493e-04, BC Loss: 4.2630e-04, IC Loss: 5.0036e-05, Data Loss: 3.1685e-05\n",
      "Iter: 18200, Loss: 6.8695e-04, PDE Loss: 3.0626e-04, BC Loss: 3.7665e-04, IC Loss: 4.0419e-06, Data Loss: 7.9989e-10\n",
      "Iter: 18400, Loss: 7.2388e-04, PDE Loss: 3.0917e-04, BC Loss: 3.8814e-04, IC Loss: 1.2384e-05, Data Loss: 1.4178e-05\n",
      "Iter: 18600, Loss: 7.1148e-04, PDE Loss: 3.0424e-04, BC Loss: 3.8470e-04, IC Loss: 1.0852e-05, Data Loss: 1.1693e-05\n",
      "Iter: 18800, Loss: 7.3085e-04, PDE Loss: 3.0408e-04, BC Loss: 3.9255e-04, IC Loss: 2.3337e-05, Data Loss: 1.0886e-05\n",
      "Iter: 19000, Loss: 6.7592e-04, PDE Loss: 3.0758e-04, BC Loss: 3.6446e-04, IC Loss: 3.7045e-06, Data Loss: 1.7211e-07\n",
      "Iter: 19200, Loss: 6.8652e-04, PDE Loss: 3.0400e-04, BC Loss: 3.7558e-04, IC Loss: 5.0540e-06, Data Loss: 1.8854e-06\n",
      "Iter: 19400, Loss: 7.6961e-04, PDE Loss: 2.9844e-04, BC Loss: 4.0146e-04, IC Loss: 3.8759e-05, Data Loss: 3.0952e-05\n",
      "Iter: 19600, Loss: 6.7990e-04, PDE Loss: 3.0193e-04, BC Loss: 3.6410e-04, IC Loss: 9.4097e-06, Data Loss: 4.4604e-06\n",
      "Iter: 19800, Loss: 6.6810e-04, PDE Loss: 3.0343e-04, BC Loss: 3.6122e-04, IC Loss: 3.3967e-06, Data Loss: 4.7995e-08\n",
      "Iter: 20000, Loss: 6.7439e-04, PDE Loss: 3.0170e-04, BC Loss: 3.6487e-04, IC Loss: 4.4767e-06, Data Loss: 3.3535e-06\n",
      "Iter: 20200, Loss: 6.6509e-04, PDE Loss: 3.0123e-04, BC Loss: 3.5768e-04, IC Loss: 6.1617e-06, Data Loss: 1.3216e-08\n",
      "Iter: 20400, Loss: 6.8614e-04, PDE Loss: 3.1554e-04, BC Loss: 3.5641e-04, IC Loss: 1.4183e-05, Data Loss: 2.8173e-09\n",
      "Iter: 20600, Loss: 6.6218e-04, PDE Loss: 3.0226e-04, BC Loss: 3.5609e-04, IC Loss: 3.7825e-06, Data Loss: 3.9378e-08\n",
      "Iter: 20800, Loss: 6.6252e-04, PDE Loss: 3.0190e-04, BC Loss: 3.5773e-04, IC Loss: 2.3876e-06, Data Loss: 5.0664e-07\n",
      "Iter: 21000, Loss: 6.6394e-04, PDE Loss: 3.0139e-04, BC Loss: 3.5459e-04, IC Loss: 6.3843e-06, Data Loss: 1.5712e-06\n",
      "Iter: 21200, Loss: 6.5722e-04, PDE Loss: 3.0375e-04, BC Loss: 3.5074e-04, IC Loss: 2.6682e-06, Data Loss: 6.5377e-08\n",
      "Iter: 21400, Loss: 6.5361e-04, PDE Loss: 2.9834e-04, BC Loss: 3.5187e-04, IC Loss: 3.3913e-06, Data Loss: 8.4777e-09\n",
      "Iter: 21600, Loss: 6.5471e-04, PDE Loss: 2.9586e-04, BC Loss: 3.5418e-04, IC Loss: 4.2191e-06, Data Loss: 4.4488e-07\n",
      "Iter: 21800, Loss: 6.5864e-04, PDE Loss: 2.9859e-04, BC Loss: 3.4963e-04, IC Loss: 1.0279e-05, Data Loss: 1.3104e-07\n",
      "Iter: 22000, Loss: 7.1120e-04, PDE Loss: 3.0568e-04, BC Loss: 3.8355e-04, IC Loss: 1.5617e-05, Data Loss: 6.3462e-06\n",
      "Iter: 22200, Loss: 6.5390e-04, PDE Loss: 2.9621e-04, BC Loss: 3.4695e-04, IC Loss: 7.1016e-06, Data Loss: 3.6385e-06\n",
      "Iter: 22400, Loss: 6.6828e-04, PDE Loss: 2.9082e-04, BC Loss: 3.5087e-04, IC Loss: 1.8497e-05, Data Loss: 8.0962e-06\n",
      "Iter: 22600, Loss: 6.5579e-04, PDE Loss: 2.9239e-04, BC Loss: 3.5714e-04, IC Loss: 4.7719e-06, Data Loss: 1.4955e-06\n",
      "Iter: 22800, Loss: 6.9678e-04, PDE Loss: 2.9143e-04, BC Loss: 3.7154e-04, IC Loss: 2.1406e-05, Data Loss: 1.2407e-05\n",
      "Iter: 23000, Loss: 6.6542e-04, PDE Loss: 2.8340e-04, BC Loss: 3.5479e-04, IC Loss: 2.0258e-05, Data Loss: 6.9732e-06\n",
      "Iter: 23200, Loss: 6.3652e-04, PDE Loss: 2.8969e-04, BC Loss: 3.4403e-04, IC Loss: 2.7641e-06, Data Loss: 4.2194e-08\n",
      "Iter: 23400, Loss: 6.3205e-04, PDE Loss: 2.8873e-04, BC Loss: 3.4029e-04, IC Loss: 3.0188e-06, Data Loss: 1.1121e-08\n",
      "Iter: 23600, Loss: 6.6893e-04, PDE Loss: 2.9727e-04, BC Loss: 3.5295e-04, IC Loss: 1.3351e-05, Data Loss: 5.3552e-06\n",
      "Iter: 23800, Loss: 6.4208e-04, PDE Loss: 2.9895e-04, BC Loss: 3.3366e-04, IC Loss: 8.4398e-06, Data Loss: 1.0408e-06\n",
      "Iter: 24000, Loss: 7.1567e-04, PDE Loss: 2.8751e-04, BC Loss: 3.8411e-04, IC Loss: 2.9574e-05, Data Loss: 1.4470e-05\n",
      "Iter: 24200, Loss: 6.2204e-04, PDE Loss: 2.8706e-04, BC Loss: 3.3125e-04, IC Loss: 3.2872e-06, Data Loss: 4.4587e-07\n",
      "Iter: 24400, Loss: 6.1663e-04, PDE Loss: 2.8573e-04, BC Loss: 3.2813e-04, IC Loss: 2.7732e-06, Data Loss: 7.5471e-11\n",
      "Iter: 24600, Loss: 6.1405e-04, PDE Loss: 2.8356e-04, BC Loss: 3.2694e-04, IC Loss: 3.1488e-06, Data Loss: 4.0690e-07\n",
      "Iter: 24800, Loss: 6.6624e-04, PDE Loss: 2.7617e-04, BC Loss: 3.5807e-04, IC Loss: 1.8281e-05, Data Loss: 1.3710e-05\n",
      "Iter: 25000, Loss: 6.1683e-04, PDE Loss: 2.8329e-04, BC Loss: 3.2655e-04, IC Loss: 6.5622e-06, Data Loss: 4.3274e-07\n",
      "Iter: 25200, Loss: 6.8928e-04, PDE Loss: 2.9579e-04, BC Loss: 3.5028e-04, IC Loss: 4.0450e-05, Data Loss: 2.7611e-06\n",
      "Iter: 25400, Loss: 5.9828e-04, PDE Loss: 2.7761e-04, BC Loss: 3.1835e-04, IC Loss: 2.3054e-06, Data Loss: 8.1408e-09\n",
      "Iter: 25600, Loss: 5.9624e-04, PDE Loss: 2.7006e-04, BC Loss: 3.1818e-04, IC Loss: 7.8098e-06, Data Loss: 1.9099e-07\n",
      "Iter: 25800, Loss: 5.9224e-04, PDE Loss: 2.7001e-04, BC Loss: 3.1400e-04, IC Loss: 5.3369e-06, Data Loss: 2.9041e-06\n",
      "Iter: 26000, Loss: 9.0661e-04, PDE Loss: 2.7313e-04, BC Loss: 3.8456e-04, IC Loss: 1.1404e-04, Data Loss: 1.3488e-04\n",
      "Iter: 26200, Loss: 6.6549e-04, PDE Loss: 2.5852e-04, BC Loss: 3.3532e-04, IC Loss: 3.8662e-05, Data Loss: 3.2980e-05\n",
      "Iter: 26400, Loss: 6.1901e-04, PDE Loss: 2.6097e-04, BC Loss: 3.1640e-04, IC Loss: 3.9899e-05, Data Loss: 1.7419e-06\n",
      "Iter: 26600, Loss: 7.2712e-04, PDE Loss: 2.6208e-04, BC Loss: 3.4070e-04, IC Loss: 5.6414e-05, Data Loss: 6.7927e-05\n",
      "Iter: 26800, Loss: 6.2165e-04, PDE Loss: 2.5916e-04, BC Loss: 3.2328e-04, IC Loss: 2.0000e-05, Data Loss: 1.9208e-05\n",
      "Iter: 27000, Loss: 5.6896e-04, PDE Loss: 2.5846e-04, BC Loss: 3.0704e-04, IC Loss: 3.4227e-06, Data Loss: 3.4495e-08\n",
      "Iter: 27200, Loss: 5.7390e-04, PDE Loss: 2.6275e-04, BC Loss: 3.0491e-04, IC Loss: 4.9371e-06, Data Loss: 1.2938e-06\n",
      "Iter: 27400, Loss: 6.6887e-04, PDE Loss: 2.5128e-04, BC Loss: 3.5999e-04, IC Loss: 4.6848e-05, Data Loss: 1.0756e-05\n",
      "Iter: 27600, Loss: 5.8059e-04, PDE Loss: 2.5313e-04, BC Loss: 3.1009e-04, IC Loss: 1.0432e-05, Data Loss: 6.9316e-06\n",
      "Iter: 27800, Loss: 5.6851e-04, PDE Loss: 2.5624e-04, BC Loss: 3.0777e-04, IC Loss: 4.0325e-06, Data Loss: 4.7063e-07\n",
      "Iter: 28000, Loss: 1.1903e-03, PDE Loss: 2.6433e-04, BC Loss: 5.5107e-04, IC Loss: 1.9559e-04, Data Loss: 1.7934e-04\n",
      "Iter: 28200, Loss: 5.5683e-04, PDE Loss: 2.5354e-04, BC Loss: 3.0105e-04, IC Loss: 2.2409e-06, Data Loss: 8.7006e-10\n",
      "Iter: 28400, Loss: 6.2511e-04, PDE Loss: 2.5068e-04, BC Loss: 3.1424e-04, IC Loss: 2.9896e-05, Data Loss: 3.0287e-05\n",
      "Iter: 28600, Loss: 5.9828e-04, PDE Loss: 2.4331e-04, BC Loss: 3.1954e-04, IC Loss: 2.3224e-05, Data Loss: 1.2213e-05\n",
      "Iter: 28800, Loss: 5.6168e-04, PDE Loss: 2.4920e-04, BC Loss: 3.0365e-04, IC Loss: 7.2063e-06, Data Loss: 1.6174e-06\n",
      "Iter: 29000, Loss: 5.6227e-04, PDE Loss: 2.5130e-04, BC Loss: 3.0699e-04, IC Loss: 3.1310e-06, Data Loss: 8.5134e-07\n",
      "Iter: 29200, Loss: 5.6543e-04, PDE Loss: 2.4486e-04, BC Loss: 3.1054e-04, IC Loss: 9.3008e-06, Data Loss: 7.3528e-07\n",
      "Iter: 29400, Loss: 5.7349e-04, PDE Loss: 2.5561e-04, BC Loss: 2.9904e-04, IC Loss: 1.0763e-05, Data Loss: 8.0731e-06\n",
      "Iter: 29600, Loss: 5.5702e-04, PDE Loss: 2.5581e-04, BC Loss: 2.9708e-04, IC Loss: 4.1221e-06, Data Loss: 4.0315e-09\n",
      "Iter: 29800, Loss: 5.5609e-04, PDE Loss: 2.4676e-04, BC Loss: 2.9796e-04, IC Loss: 1.1364e-05, Data Loss: 9.8061e-09\n",
      "Iter: 30000, Loss: 5.5098e-04, PDE Loss: 2.4877e-04, BC Loss: 2.9813e-04, IC Loss: 3.9449e-06, Data Loss: 1.4298e-07\n",
      "Iter: 30200, Loss: 5.5460e-04, PDE Loss: 2.4386e-04, BC Loss: 2.9394e-04, IC Loss: 1.6624e-05, Data Loss: 1.8303e-07\n",
      "Iter: 30400, Loss: 5.4181e-04, PDE Loss: 2.4600e-04, BC Loss: 2.9341e-04, IC Loss: 2.3787e-06, Data Loss: 1.9591e-08\n",
      "Iter: 30600, Loss: 5.4042e-04, PDE Loss: 2.4731e-04, BC Loss: 2.9126e-04, IC Loss: 1.8351e-06, Data Loss: 1.4073e-08\n",
      "Iter: 30800, Loss: 8.5972e-04, PDE Loss: 2.7158e-04, BC Loss: 4.3920e-04, IC Loss: 1.4676e-04, Data Loss: 2.1771e-06\n",
      "Iter: 31000, Loss: 5.7704e-04, PDE Loss: 2.6474e-04, BC Loss: 2.9066e-04, IC Loss: 1.4585e-05, Data Loss: 7.0565e-06\n",
      "Iter: 31200, Loss: 5.4203e-04, PDE Loss: 2.4588e-04, BC Loss: 2.9114e-04, IC Loss: 3.2452e-06, Data Loss: 1.7721e-06\n",
      "Iter: 31400, Loss: 5.3558e-04, PDE Loss: 2.4174e-04, BC Loss: 2.9066e-04, IC Loss: 2.7431e-06, Data Loss: 4.4646e-07\n",
      "Iter: 31600, Loss: 5.3371e-04, PDE Loss: 2.4229e-04, BC Loss: 2.8958e-04, IC Loss: 1.8098e-06, Data Loss: 2.5631e-08\n",
      "Iter: 31800, Loss: 5.5077e-04, PDE Loss: 2.4065e-04, BC Loss: 3.0025e-04, IC Loss: 9.3130e-06, Data Loss: 5.6040e-07\n",
      "Iter: 32000, Loss: 5.6166e-04, PDE Loss: 2.3897e-04, BC Loss: 3.0722e-04, IC Loss: 1.1310e-05, Data Loss: 4.1628e-06\n",
      "Iter: 32200, Loss: 5.5156e-04, PDE Loss: 2.3105e-04, BC Loss: 3.0954e-04, IC Loss: 1.0710e-05, Data Loss: 2.5340e-07\n",
      "Iter: 32400, Loss: 5.8224e-04, PDE Loss: 2.2610e-04, BC Loss: 3.2991e-04, IC Loss: 1.9645e-05, Data Loss: 6.5812e-06\n",
      "Iter: 32600, Loss: 6.3191e-04, PDE Loss: 2.2987e-04, BC Loss: 3.1435e-04, IC Loss: 6.1920e-05, Data Loss: 2.5773e-05\n",
      "Iter: 32800, Loss: 6.1063e-04, PDE Loss: 2.3655e-04, BC Loss: 3.5433e-04, IC Loss: 1.9059e-05, Data Loss: 6.9031e-07\n",
      "Iter: 33000, Loss: 5.2806e-04, PDE Loss: 2.3104e-04, BC Loss: 2.9418e-04, IC Loss: 2.6664e-06, Data Loss: 1.7918e-07\n",
      "Iter: 33200, Loss: 5.3995e-04, PDE Loss: 2.3449e-04, BC Loss: 2.9774e-04, IC Loss: 6.1262e-06, Data Loss: 1.5993e-06\n",
      "Iter: 33400, Loss: 5.2211e-04, PDE Loss: 2.3564e-04, BC Loss: 2.8511e-04, IC Loss: 1.3552e-06, Data Loss: 1.3805e-10\n",
      "Iter: 33600, Loss: 6.8353e-04, PDE Loss: 2.4343e-04, BC Loss: 3.0598e-04, IC Loss: 6.2129e-05, Data Loss: 7.1998e-05\n",
      "Iter: 33800, Loss: 5.1962e-04, PDE Loss: 2.3157e-04, BC Loss: 2.8640e-04, IC Loss: 1.5897e-06, Data Loss: 5.0890e-08\n",
      "Iter: 34000, Loss: 5.5499e-04, PDE Loss: 2.3821e-04, BC Loss: 2.9205e-04, IC Loss: 1.9491e-05, Data Loss: 5.2366e-06\n",
      "Iter: 34200, Loss: 5.4205e-04, PDE Loss: 2.3417e-04, BC Loss: 3.0042e-04, IC Loss: 5.6048e-06, Data Loss: 1.8597e-06\n",
      "Iter: 34400, Loss: 7.6332e-04, PDE Loss: 2.3728e-04, BC Loss: 3.3507e-04, IC Loss: 9.9921e-05, Data Loss: 9.1054e-05\n",
      "Iter: 34600, Loss: 5.2485e-04, PDE Loss: 2.3435e-04, BC Loss: 2.8631e-04, IC Loss: 3.9354e-06, Data Loss: 2.5749e-07\n",
      "Iter: 34800, Loss: 5.4882e-04, PDE Loss: 2.3206e-04, BC Loss: 2.8853e-04, IC Loss: 1.4456e-05, Data Loss: 1.3764e-05\n",
      "Iter: 35000, Loss: 5.1902e-04, PDE Loss: 2.2793e-04, BC Loss: 2.8668e-04, IC Loss: 3.0649e-06, Data Loss: 1.3520e-06\n",
      "Iter: 35200, Loss: 6.6663e-04, PDE Loss: 2.1136e-04, BC Loss: 3.4332e-04, IC Loss: 1.1170e-04, Data Loss: 2.4405e-07\n",
      "Iter: 35400, Loss: 5.6956e-04, PDE Loss: 2.2786e-04, BC Loss: 2.9168e-04, IC Loss: 2.3605e-05, Data Loss: 2.6411e-05\n",
      "Iter: 35600, Loss: 5.7851e-04, PDE Loss: 2.3215e-04, BC Loss: 3.3427e-04, IC Loss: 1.1756e-05, Data Loss: 3.2686e-07\n",
      "Iter: 35800, Loss: 5.0792e-04, PDE Loss: 2.2648e-04, BC Loss: 2.7902e-04, IC Loss: 2.0678e-06, Data Loss: 3.5198e-07\n",
      "Iter: 36000, Loss: 6.9589e-04, PDE Loss: 2.2266e-04, BC Loss: 3.8133e-04, IC Loss: 5.5375e-05, Data Loss: 3.6524e-05\n",
      "Iter: 36200, Loss: 5.0299e-04, PDE Loss: 2.2380e-04, BC Loss: 2.7759e-04, IC Loss: 1.4147e-06, Data Loss: 1.8772e-07\n",
      "Iter: 36400, Loss: 6.1762e-04, PDE Loss: 2.4754e-04, BC Loss: 3.1614e-04, IC Loss: 3.2133e-05, Data Loss: 2.1815e-05\n",
      "Iter: 36600, Loss: 5.3641e-04, PDE Loss: 2.2388e-04, BC Loss: 2.8826e-04, IC Loss: 1.2181e-05, Data Loss: 1.2087e-05\n",
      "Iter: 36800, Loss: 4.9990e-04, PDE Loss: 2.1977e-04, BC Loss: 2.7846e-04, IC Loss: 1.4533e-06, Data Loss: 2.1098e-07\n",
      "Iter: 37000, Loss: 6.6551e-04, PDE Loss: 2.3788e-04, BC Loss: 2.9871e-04, IC Loss: 8.7697e-05, Data Loss: 4.1217e-05\n",
      "Iter: 37200, Loss: 5.2405e-04, PDE Loss: 2.2820e-04, BC Loss: 2.8339e-04, IC Loss: 1.0600e-05, Data Loss: 1.8609e-06\n",
      "Iter: 37400, Loss: 5.6372e-04, PDE Loss: 2.1682e-04, BC Loss: 2.8888e-04, IC Loss: 3.2719e-05, Data Loss: 2.5307e-05\n",
      "Iter: 37600, Loss: 4.9231e-04, PDE Loss: 2.1662e-04, BC Loss: 2.7365e-04, IC Loss: 2.0463e-06, Data Loss: 2.8353e-12\n",
      "Iter: 37800, Loss: 4.9205e-04, PDE Loss: 2.1835e-04, BC Loss: 2.7050e-04, IC Loss: 2.9938e-06, Data Loss: 2.0011e-07\n",
      "Iter: 38000, Loss: 4.8911e-04, PDE Loss: 2.1735e-04, BC Loss: 2.7045e-04, IC Loss: 1.3039e-06, Data Loss: 9.6167e-09\n",
      "Iter: 38200, Loss: 4.8704e-04, PDE Loss: 2.1274e-04, BC Loss: 2.7180e-04, IC Loss: 2.4906e-06, Data Loss: 3.3007e-09\n",
      "Iter: 38400, Loss: 4.9234e-04, PDE Loss: 2.1958e-04, BC Loss: 2.6809e-04, IC Loss: 4.4078e-06, Data Loss: 2.5735e-07\n",
      "Iter: 38600, Loss: 4.9398e-04, PDE Loss: 2.1815e-04, BC Loss: 2.7099e-04, IC Loss: 4.6757e-06, Data Loss: 1.6497e-07\n",
      "Iter: 38800, Loss: 5.0344e-04, PDE Loss: 2.0662e-04, BC Loss: 2.7790e-04, IC Loss: 1.3490e-05, Data Loss: 5.4231e-06\n",
      "Iter: 39000, Loss: 4.8797e-04, PDE Loss: 2.1112e-04, BC Loss: 2.6768e-04, IC Loss: 5.2421e-06, Data Loss: 3.9297e-06\n",
      "Iter: 39200, Loss: 6.2702e-04, PDE Loss: 2.5045e-04, BC Loss: 2.9773e-04, IC Loss: 4.1449e-05, Data Loss: 3.7398e-05\n",
      "Iter: 39400, Loss: 5.4942e-04, PDE Loss: 2.0999e-04, BC Loss: 2.8232e-04, IC Loss: 3.0262e-05, Data Loss: 2.6850e-05\n",
      "Iter: 39600, Loss: 4.7014e-04, PDE Loss: 2.0561e-04, BC Loss: 2.6309e-04, IC Loss: 1.4392e-06, Data Loss: 3.4169e-10\n",
      "Iter: 39800, Loss: 5.0721e-04, PDE Loss: 2.1520e-04, BC Loss: 2.6667e-04, IC Loss: 2.4109e-05, Data Loss: 1.2232e-06\n",
      "Iter: 40000, Loss: 5.2982e-04, PDE Loss: 1.9943e-04, BC Loss: 3.2091e-04, IC Loss: 5.7006e-06, Data Loss: 3.7825e-06\n",
      "Iter: 40200, Loss: 4.6242e-04, PDE Loss: 2.0186e-04, BC Loss: 2.5835e-04, IC Loss: 2.1675e-06, Data Loss: 3.4010e-08\n",
      "Iter: 40400, Loss: 5.5208e-04, PDE Loss: 2.1045e-04, BC Loss: 2.7120e-04, IC Loss: 3.9098e-05, Data Loss: 3.1328e-05\n",
      "Iter: 40600, Loss: 5.3815e-04, PDE Loss: 2.2572e-04, BC Loss: 2.6057e-04, IC Loss: 4.7629e-05, Data Loss: 4.2284e-06\n",
      "Iter: 40800, Loss: 4.6678e-04, PDE Loss: 1.9151e-04, BC Loss: 2.6880e-04, IC Loss: 5.9047e-06, Data Loss: 5.6819e-07\n",
      "Iter: 41000, Loss: 4.8731e-04, PDE Loss: 1.9588e-04, BC Loss: 2.6176e-04, IC Loss: 1.7918e-05, Data Loss: 1.1757e-05\n",
      "Iter: 41200, Loss: 4.4811e-04, PDE Loss: 1.9666e-04, BC Loss: 2.5007e-04, IC Loss: 1.3189e-06, Data Loss: 5.3312e-08\n",
      "Iter: 41400, Loss: 4.4490e-04, PDE Loss: 1.9366e-04, BC Loss: 2.4822e-04, IC Loss: 2.4337e-06, Data Loss: 5.7764e-07\n",
      "Iter: 41600, Loss: 7.9135e-04, PDE Loss: 2.0854e-04, BC Loss: 3.6357e-04, IC Loss: 1.0373e-04, Data Loss: 1.1551e-04\n",
      "Iter: 41800, Loss: 4.3991e-04, PDE Loss: 1.9045e-04, BC Loss: 2.4695e-04, IC Loss: 1.7545e-06, Data Loss: 7.4937e-07\n",
      "Iter: 42000, Loss: 5.1103e-04, PDE Loss: 2.0202e-04, BC Loss: 2.7017e-04, IC Loss: 2.6779e-05, Data Loss: 1.2066e-05\n",
      "Iter: 42200, Loss: 6.5077e-04, PDE Loss: 1.9378e-04, BC Loss: 3.2349e-04, IC Loss: 6.9113e-05, Data Loss: 6.4390e-05\n",
      "Iter: 42400, Loss: 6.0647e-04, PDE Loss: 2.2258e-04, BC Loss: 2.5441e-04, IC Loss: 9.8885e-05, Data Loss: 3.0591e-05\n",
      "Iter: 42600, Loss: 4.4404e-04, PDE Loss: 1.8532e-04, BC Loss: 2.4703e-04, IC Loss: 5.5527e-06, Data Loss: 6.1356e-06\n",
      "Iter: 42800, Loss: 4.3254e-04, PDE Loss: 1.8679e-04, BC Loss: 2.4063e-04, IC Loss: 3.6914e-06, Data Loss: 1.4315e-06\n",
      "Iter: 43000, Loss: 4.7698e-04, PDE Loss: 1.8593e-04, BC Loss: 2.6511e-04, IC Loss: 2.1608e-05, Data Loss: 4.3265e-06\n",
      "Iter: 43200, Loss: 4.8093e-04, PDE Loss: 1.9915e-04, BC Loss: 2.5252e-04, IC Loss: 2.5733e-05, Data Loss: 3.5226e-06\n",
      "Iter: 43400, Loss: 4.6800e-04, PDE Loss: 1.9499e-04, BC Loss: 2.5223e-04, IC Loss: 2.0441e-05, Data Loss: 3.4206e-07\n",
      "Iter: 43600, Loss: 4.6202e-04, PDE Loss: 1.8567e-04, BC Loss: 2.3438e-04, IC Loss: 2.7114e-05, Data Loss: 1.4864e-05\n",
      "Iter: 43800, Loss: 5.5349e-04, PDE Loss: 2.0191e-04, BC Loss: 2.5847e-04, IC Loss: 6.7136e-05, Data Loss: 2.5980e-05\n",
      "Iter: 44000, Loss: 4.5480e-04, PDE Loss: 1.8156e-04, BC Loss: 2.4191e-04, IC Loss: 1.4695e-05, Data Loss: 1.6633e-05\n",
      "Iter: 44200, Loss: 4.2040e-04, PDE Loss: 1.7720e-04, BC Loss: 2.3107e-04, IC Loss: 5.8848e-06, Data Loss: 6.2572e-06\n",
      "Iter: 44400, Loss: 4.0450e-04, PDE Loss: 1.7794e-04, BC Loss: 2.2537e-04, IC Loss: 1.1696e-06, Data Loss: 1.8743e-08\n",
      "Iter: 44600, Loss: 4.2299e-04, PDE Loss: 1.8487e-04, BC Loss: 2.3186e-04, IC Loss: 3.9959e-06, Data Loss: 2.2707e-06\n",
      "Iter: 44800, Loss: 4.0420e-04, PDE Loss: 1.7874e-04, BC Loss: 2.2159e-04, IC Loss: 2.5973e-06, Data Loss: 1.2657e-06\n",
      "Iter: 45000, Loss: 4.1085e-04, PDE Loss: 1.8192e-04, BC Loss: 2.2579e-04, IC Loss: 3.0469e-06, Data Loss: 8.7976e-08\n",
      "Iter: 45200, Loss: 4.3462e-04, PDE Loss: 1.7310e-04, BC Loss: 2.3681e-04, IC Loss: 1.4007e-05, Data Loss: 1.0710e-05\n",
      "Iter: 45400, Loss: 4.5458e-04, PDE Loss: 1.8774e-04, BC Loss: 2.5261e-04, IC Loss: 7.7648e-06, Data Loss: 6.4586e-06\n",
      "Iter: 45600, Loss: 4.2606e-04, PDE Loss: 1.7649e-04, BC Loss: 2.2042e-04, IC Loss: 2.0730e-05, Data Loss: 8.4145e-06\n",
      "Iter: 45800, Loss: 4.0021e-04, PDE Loss: 1.7471e-04, BC Loss: 2.1615e-04, IC Loss: 5.8773e-06, Data Loss: 3.4782e-06\n",
      "Iter: 46000, Loss: 3.9698e-04, PDE Loss: 1.7376e-04, BC Loss: 2.1608e-04, IC Loss: 5.9194e-06, Data Loss: 1.2176e-06\n",
      "Iter: 46200, Loss: 4.2073e-04, PDE Loss: 1.6986e-04, BC Loss: 2.3035e-04, IC Loss: 1.2037e-05, Data Loss: 8.4832e-06\n",
      "Iter: 46400, Loss: 1.2266e-02, PDE Loss: 7.4583e-03, BC Loss: 1.5784e-03, IC Loss: 3.0200e-03, Data Loss: 2.0927e-04\n",
      "Iter: 46600, Loss: 8.0410e-04, PDE Loss: 4.1372e-04, BC Loss: 3.7509e-04, IC Loss: 1.5196e-05, Data Loss: 9.4163e-08\n",
      "Iter: 46800, Loss: 6.7936e-04, PDE Loss: 3.1770e-04, BC Loss: 3.5587e-04, IC Loss: 5.7330e-06, Data Loss: 5.0961e-08\n",
      "Iter: 47000, Loss: 6.2392e-04, PDE Loss: 2.8254e-04, BC Loss: 3.3753e-04, IC Loss: 3.8205e-06, Data Loss: 3.3055e-08\n",
      "Iter: 47200, Loss: 8.1907e-04, PDE Loss: 2.6330e-04, BC Loss: 3.2484e-04, IC Loss: 1.1685e-04, Data Loss: 1.1409e-04\n",
      "Iter: 47400, Loss: 5.6165e-04, PDE Loss: 2.5209e-04, BC Loss: 3.0692e-04, IC Loss: 2.6203e-06, Data Loss: 1.4507e-08\n",
      "Iter: 47600, Loss: 5.3958e-04, PDE Loss: 2.4326e-04, BC Loss: 2.9391e-04, IC Loss: 2.4100e-06, Data Loss: 2.5255e-09\n",
      "Iter: 47800, Loss: 5.2017e-04, PDE Loss: 2.3615e-04, BC Loss: 2.8189e-04, IC Loss: 1.8222e-06, Data Loss: 3.1418e-07\n",
      "Iter: 48000, Loss: 5.0314e-04, PDE Loss: 2.3011e-04, BC Loss: 2.7111e-04, IC Loss: 1.9214e-06, Data Loss: 4.3076e-09\n",
      "Iter: 48200, Loss: 4.8871e-04, PDE Loss: 2.2490e-04, BC Loss: 2.6178e-04, IC Loss: 2.0089e-06, Data Loss: 1.9385e-08\n",
      "Iter: 48400, Loss: 4.8741e-04, PDE Loss: 2.2061e-04, BC Loss: 2.5549e-04, IC Loss: 4.9598e-06, Data Loss: 6.3414e-06\n",
      "Iter: 48600, Loss: 4.6392e-04, PDE Loss: 2.1575e-04, BC Loss: 2.4661e-04, IC Loss: 1.5565e-06, Data Loss: 1.3355e-09\n",
      "Iter: 48800, Loss: 4.5553e-04, PDE Loss: 2.1228e-04, BC Loss: 2.4090e-04, IC Loss: 2.1140e-06, Data Loss: 2.3913e-07\n",
      "Iter: 49000, Loss: 4.4526e-04, PDE Loss: 2.0761e-04, BC Loss: 2.3629e-04, IC Loss: 1.3505e-06, Data Loss: 3.1497e-10\n",
      "Iter: 49200, Loss: 4.3835e-04, PDE Loss: 2.0438e-04, BC Loss: 2.3267e-04, IC Loss: 1.3019e-06, Data Loss: 6.9758e-11\n",
      "Iter: 49400, Loss: 4.3317e-04, PDE Loss: 2.0154e-04, BC Loss: 2.2939e-04, IC Loss: 1.8373e-06, Data Loss: 4.0782e-07\n",
      "Iter: 49600, Loss: 4.2677e-04, PDE Loss: 1.9796e-04, BC Loss: 2.2752e-04, IC Loss: 1.2826e-06, Data Loss: 9.5016e-09\n",
      "Iter: 49800, Loss: 4.2942e-04, PDE Loss: 1.9442e-04, BC Loss: 2.2906e-04, IC Loss: 2.9167e-06, Data Loss: 3.0213e-06\n",
      "Iter: 50000, Loss: 4.1883e-04, PDE Loss: 1.9245e-04, BC Loss: 2.2466e-04, IC Loss: 1.5880e-06, Data Loss: 1.3851e-07\n",
      "Iter: 50200, Loss: 4.1422e-04, PDE Loss: 1.8970e-04, BC Loss: 2.2334e-04, IC Loss: 1.1752e-06, Data Loss: 2.9420e-09\n",
      "Iter: 50400, Loss: 4.1165e-04, PDE Loss: 1.8757e-04, BC Loss: 2.2232e-04, IC Loss: 1.4425e-06, Data Loss: 3.1984e-07\n",
      "Iter: 50600, Loss: 4.0946e-04, PDE Loss: 1.8423e-04, BC Loss: 2.2237e-04, IC Loss: 2.8589e-06, Data Loss: 7.6536e-09\n",
      "Iter: 50800, Loss: 4.0721e-04, PDE Loss: 1.8254e-04, BC Loss: 2.2239e-04, IC Loss: 1.6886e-06, Data Loss: 5.9543e-07\n",
      "Iter: 51000, Loss: 4.1126e-04, PDE Loss: 1.8121e-04, BC Loss: 2.2487e-04, IC Loss: 3.1411e-06, Data Loss: 2.0399e-06\n",
      "Iter: 51200, Loss: 4.0093e-04, PDE Loss: 1.7845e-04, BC Loss: 2.2022e-04, IC Loss: 1.8818e-06, Data Loss: 3.7603e-07\n",
      "Iter: 51400, Loss: 4.0347e-04, PDE Loss: 1.7627e-04, BC Loss: 2.2351e-04, IC Loss: 3.2876e-06, Data Loss: 3.9941e-07\n",
      "Iter: 51600, Loss: 4.0889e-04, PDE Loss: 1.7873e-04, BC Loss: 2.2155e-04, IC Loss: 7.3069e-06, Data Loss: 1.2971e-06\n",
      "Iter: 51800, Loss: 4.6688e-04, PDE Loss: 1.9404e-04, BC Loss: 2.3212e-04, IC Loss: 3.9770e-05, Data Loss: 9.4966e-07\n",
      "Iter: 52000, Loss: 3.9360e-04, PDE Loss: 1.7127e-04, BC Loss: 2.1786e-04, IC Loss: 4.0323e-06, Data Loss: 4.3406e-07\n",
      "Iter: 52200, Loss: 3.8811e-04, PDE Loss: 1.7041e-04, BC Loss: 2.1680e-04, IC Loss: 8.6292e-07, Data Loss: 3.9405e-08\n",
      "Iter: 52400, Loss: 4.1118e-04, PDE Loss: 1.6973e-04, BC Loss: 2.1917e-04, IC Loss: 1.2341e-05, Data Loss: 9.9352e-06\n",
      "Iter: 52600, Loss: 4.0924e-04, PDE Loss: 1.7394e-04, BC Loss: 2.2938e-04, IC Loss: 4.6507e-06, Data Loss: 1.2681e-06\n",
      "Iter: 52800, Loss: 5.0384e-04, PDE Loss: 1.6783e-04, BC Loss: 2.4026e-04, IC Loss: 4.9206e-05, Data Loss: 4.6544e-05\n",
      "Iter: 53000, Loss: 3.8121e-04, PDE Loss: 1.6548e-04, BC Loss: 2.1438e-04, IC Loss: 1.1981e-06, Data Loss: 1.5530e-07\n",
      "Iter: 53200, Loss: 3.7882e-04, PDE Loss: 1.6235e-04, BC Loss: 2.1402e-04, IC Loss: 1.8116e-06, Data Loss: 6.4232e-07\n",
      "Iter: 53400, Loss: 3.8260e-04, PDE Loss: 1.6261e-04, BC Loss: 2.1723e-04, IC Loss: 2.5627e-06, Data Loss: 1.9605e-07\n",
      "Iter: 53600, Loss: 4.2402e-04, PDE Loss: 1.6509e-04, BC Loss: 2.2873e-04, IC Loss: 1.5061e-05, Data Loss: 1.5145e-05\n",
      "Iter: 53800, Loss: 3.7377e-04, PDE Loss: 1.5984e-04, BC Loss: 2.1188e-04, IC Loss: 2.0512e-06, Data Loss: 8.7850e-09\n",
      "Iter: 54000, Loss: 3.7635e-04, PDE Loss: 1.5933e-04, BC Loss: 2.1159e-04, IC Loss: 3.1397e-06, Data Loss: 2.2915e-06\n",
      "Iter: 54200, Loss: 4.0483e-04, PDE Loss: 1.5886e-04, BC Loss: 2.1194e-04, IC Loss: 1.5721e-05, Data Loss: 1.8314e-05\n",
      "Iter: 54400, Loss: 3.7613e-04, PDE Loss: 1.5517e-04, BC Loss: 2.0839e-04, IC Loss: 1.1088e-05, Data Loss: 1.4782e-06\n",
      "Iter: 54600, Loss: 3.8796e-04, PDE Loss: 1.6583e-04, BC Loss: 2.1268e-04, IC Loss: 8.5847e-06, Data Loss: 8.6279e-07\n",
      "Iter: 54800, Loss: 3.6801e-04, PDE Loss: 1.5470e-04, BC Loss: 2.0985e-04, IC Loss: 2.3234e-06, Data Loss: 1.1303e-06\n",
      "Iter: 55000, Loss: 3.6052e-04, PDE Loss: 1.5017e-04, BC Loss: 2.0453e-04, IC Loss: 3.5617e-06, Data Loss: 2.2554e-06\n",
      "Iter: 55200, Loss: 3.6710e-04, PDE Loss: 1.5179e-04, BC Loss: 2.0320e-04, IC Loss: 9.4890e-06, Data Loss: 2.6237e-06\n",
      "Iter: 55400, Loss: 3.4680e-04, PDE Loss: 1.4480e-04, BC Loss: 1.9892e-04, IC Loss: 1.6750e-06, Data Loss: 1.4060e-06\n",
      "Iter: 55600, Loss: 3.4367e-04, PDE Loss: 1.4632e-04, BC Loss: 1.9601e-04, IC Loss: 1.1018e-06, Data Loss: 2.2996e-07\n",
      "Iter: 55800, Loss: 4.3334e-04, PDE Loss: 1.4861e-04, BC Loss: 2.1563e-04, IC Loss: 3.2397e-05, Data Loss: 3.6703e-05\n",
      "Iter: 56000, Loss: 3.3385e-04, PDE Loss: 1.3930e-04, BC Loss: 1.9224e-04, IC Loss: 2.2591e-06, Data Loss: 5.1392e-08\n",
      "Iter: 56200, Loss: 3.5139e-04, PDE Loss: 1.4766e-04, BC Loss: 1.9678e-04, IC Loss: 4.9972e-06, Data Loss: 1.9472e-06\n",
      "Iter: 56400, Loss: 3.3112e-04, PDE Loss: 1.3830e-04, BC Loss: 1.8735e-04, IC Loss: 3.9498e-06, Data Loss: 1.5210e-06\n",
      "Iter: 56600, Loss: 4.5993e-04, PDE Loss: 1.5989e-04, BC Loss: 2.5847e-04, IC Loss: 3.2116e-05, Data Loss: 9.4514e-06\n",
      "Iter: 56800, Loss: 3.1443e-04, PDE Loss: 1.3086e-04, BC Loss: 1.8290e-04, IC Loss: 6.5286e-07, Data Loss: 1.0323e-08\n",
      "Iter: 57000, Loss: 3.4584e-04, PDE Loss: 1.5677e-04, BC Loss: 1.8710e-04, IC Loss: 1.6752e-06, Data Loss: 2.9492e-07\n",
      "Iter: 57200, Loss: 3.2363e-04, PDE Loss: 1.2971e-04, BC Loss: 1.8944e-04, IC Loss: 2.8056e-06, Data Loss: 1.6825e-06\n",
      "Iter: 57400, Loss: 3.4720e-04, PDE Loss: 1.4520e-04, BC Loss: 1.9566e-04, IC Loss: 5.2409e-06, Data Loss: 1.1056e-06\n",
      "Iter: 57600, Loss: 3.6081e-04, PDE Loss: 1.3068e-04, BC Loss: 2.2846e-04, IC Loss: 1.6145e-06, Data Loss: 5.8648e-08\n",
      "Iter: 57800, Loss: 3.0697e-04, PDE Loss: 1.2472e-04, BC Loss: 1.7943e-04, IC Loss: 2.4349e-06, Data Loss: 3.8815e-07\n",
      "Iter: 58000, Loss: 3.2131e-04, PDE Loss: 1.2731e-04, BC Loss: 1.8474e-04, IC Loss: 4.5750e-06, Data Loss: 4.6828e-06\n",
      "Iter: 58200, Loss: 3.2620e-04, PDE Loss: 1.3044e-04, BC Loss: 1.8659e-04, IC Loss: 8.7523e-06, Data Loss: 4.2504e-07\n",
      "Iter: 58400, Loss: 3.7329e-04, PDE Loss: 1.4755e-04, BC Loss: 1.9571e-04, IC Loss: 1.8423e-05, Data Loss: 1.1616e-05\n",
      "Iter: 58600, Loss: 3.0544e-04, PDE Loss: 1.2171e-04, BC Loss: 1.7862e-04, IC Loss: 4.0635e-06, Data Loss: 1.0419e-06\n",
      "Iter: 58800, Loss: 3.1543e-04, PDE Loss: 1.2377e-04, BC Loss: 1.8397e-04, IC Loss: 6.1313e-06, Data Loss: 1.5532e-06\n",
      "Iter: 59000, Loss: 2.9938e-04, PDE Loss: 1.2079e-04, BC Loss: 1.7621e-04, IC Loss: 1.6101e-06, Data Loss: 7.6960e-07\n",
      "Iter: 59200, Loss: 3.1816e-04, PDE Loss: 1.2215e-04, BC Loss: 1.8065e-04, IC Loss: 1.1195e-05, Data Loss: 4.1604e-06\n",
      "Iter: 59400, Loss: 3.6223e-04, PDE Loss: 1.2664e-04, BC Loss: 1.8446e-04, IC Loss: 3.1179e-05, Data Loss: 1.9948e-05\n",
      "Iter: 59600, Loss: 3.8557e-04, PDE Loss: 1.6531e-04, BC Loss: 2.0154e-04, IC Loss: 1.8437e-05, Data Loss: 2.7509e-07\n",
      "Iter: 59800, Loss: 3.7525e-04, PDE Loss: 1.5378e-04, BC Loss: 1.9165e-04, IC Loss: 2.9789e-05, Data Loss: 3.1926e-08\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self,data, layers, act=nn.Tanh()):\n",
    "        super().__init__()\n",
    "        self.data = {k: v.to(device) for k, v in data.items()}\n",
    "        self.layers = layers\n",
    "        self.act = act\n",
    "        \n",
    "        modules = []\n",
    "        for i in range(len(layers) - 2):\n",
    "            modules.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "            nn.init.xavier_normal_(modules[-1].weight)\n",
    "            modules.append(self.act)\n",
    "        \n",
    "        modules.append(nn.Linear(layers[-2], layers[-1]))\n",
    "        nn.init.xavier_normal_(modules[-1].weight)\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.net(torch.cat([x, t], dim=1))\n",
    "\n",
    "    def compute_PDE_loss(self,c=1):\n",
    "        x_f = self.prep_tensor(self.data[\"x_f\"], requires_grad=True)\n",
    "        t_f = self.prep_tensor(self.data[\"t_f\"], requires_grad=True)\n",
    "\n",
    "        u_f = self(x_f,t_f)\n",
    "\n",
    "        u_dx, u_dt = grad(u_f, (x_f, t_f), torch.ones_like(u_f),retain_graph=True, create_graph=True)\n",
    "        u_ddx = grad(u_dx, x_f, torch.ones_like(u_f),create_graph=True)[0]\n",
    "        u_ddt = grad(u_dt, t_f, torch.ones_like(u_f),create_graph=True)[0]\n",
    "\n",
    "        residual = u_ddt - u_ddx*c**2\n",
    "        residual = torch.mean(residual**2)\n",
    "    \n",
    "        return residual\n",
    "\n",
    "    def compute_boundary_loss(self):\n",
    "        x_bc = self.prep_tensor(self.data[\"x_bc\"])\n",
    "        t_bc = self.prep_tensor(self.data[\"t_bc\"])\n",
    "        u_bc = self.prep_tensor(self.data[\"u_bc\"])\n",
    "\n",
    "        u_bc_pred = self(x_bc,t_bc)\n",
    "        bc_loss = torch.mean((u_bc_pred - u_bc)**2)\n",
    "        return bc_loss\n",
    "    \n",
    "    def compute_initial_condition_loss(self):\n",
    "        x_ic = self.prep_tensor(self.data[\"x_ic\"], requires_grad=True)\n",
    "        t_ic = self.prep_tensor(self.data[\"t_ic\"], requires_grad=True)\n",
    "        u_ic = self.prep_tensor(self.data[\"u_ic\"], requires_grad=True)\n",
    "        u_t_ic = self.prep_tensor(self.data[\"u_t_ic\"])\n",
    "\n",
    "        u_ic_pred = self(x_ic,t_ic)\n",
    "        u_t = grad(u_ic_pred, t_ic, torch.ones_like(u_ic_pred), create_graph=True)[0]\n",
    "        ic_loss = torch.mean((u_ic_pred - u_ic)**2) + torch.mean((u_t - u_t_ic)**2)\n",
    "        return ic_loss\n",
    "\n",
    "    def compute_data_loss(self):\n",
    "        x_data = self.prep_tensor(self.data[\"x_data\"], requires_grad=True)\n",
    "        t_data = self.prep_tensor(self.data[\"t_data\"], requires_grad=True)\n",
    "        u_data = self.prep_tensor(self.data[\"u_data\"])\n",
    "\n",
    "        u_data_pred = self(x_data,t_data)\n",
    "        data_loss = torch.mean((u_data_pred - u_data)**2)\n",
    "        return data_loss\n",
    "\n",
    "    def compute_total_loss(self, lambda_f=1.0, lambda_bc=1.0, lambda_ic=1.0, lambda_data=1.0):\n",
    "        pde_loss = self.compute_PDE_loss()*lambda_f\n",
    "        bc_loss = self.compute_boundary_loss()*lambda_bc\n",
    "        ic_loss = self.compute_initial_condition_loss()*lambda_ic\n",
    "        data_loss = self.compute_data_loss()*lambda_data\n",
    "        \n",
    "        total_loss = lambda_f*pde_loss + lambda_bc*bc_loss + lambda_ic*ic_loss + lambda_data*data_loss\n",
    "        \n",
    "        loss_for_display = {\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"pde_loss\": lambda_f*pde_loss.item(),\n",
    "            \"bc_loss\": lambda_bc*bc_loss.item(),\n",
    "            \"ic_loss\": lambda_ic*ic_loss.item(),\n",
    "            \"data_loss\": lambda_data*data_loss.item()}\n",
    "        \n",
    "        return total_loss, loss_for_display\n",
    "\n",
    "    def prep_tensor(self, x, requires_grad=False):\n",
    "        return x.clone().detach().to(device).requires_grad_(requires_grad)\n",
    "\n",
    "    def train_model(self, niter):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        for it in range(niter):\n",
    "            optimizer.zero_grad()\n",
    "            loss, loss_for_display = self.compute_total_loss()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # break\n",
    "            if it % 200 == 0:\n",
    "                print(f\"Iter: {it}, Loss: {loss.item():.4e}, PDE Loss: {loss_for_display['pde_loss']:.4e}, BC Loss: {loss_for_display['bc_loss']:.4e}, IC Loss: {loss_for_display['ic_loss']:.4e}, Data Loss: {loss_for_display['data_loss']:.4e}\")\n",
    "    \n",
    "    def predict(self, x, t):\n",
    "        x = x.clone().detach().to(device)\n",
    "        t = t.clone().detach().to(device)\n",
    "        u = self(x,t)\n",
    "        return u.detach().cpu().numpy()\n",
    "\n",
    "N_bc = 100\n",
    "N_ic = 100\n",
    "N_f = 20000\n",
    "t_end = 12*np.pi\n",
    "# Define boundary conditions\n",
    "x_bc = torch.cat([torch.zeros(N_bc, 1), torch.ones(N_bc, 1)])*np.pi\n",
    "t_bc = torch.rand(2*N_bc, 1, requires_grad=True)*t_end\n",
    "u_bc = torch.ones((2*N_bc, 1))*0.0\n",
    "\n",
    "# Define initial conditions\n",
    "x_ic = torch.rand(N_ic, 1)*np.pi\n",
    "t_ic = torch.zeros(N_ic, 1)\n",
    "u_ic = torch.zeros(N_ic, 1)\n",
    "u_t_ic = torch.sin(x_ic)\n",
    "\n",
    "#Define PDE collocation points\n",
    "def u_sol(t,x,n,Kn = 1,l = np.pi,c = 1):\n",
    "        return Kn*np.sin(n*np.pi/l*(c*t))*np.sin(n*np.pi/l*x)\n",
    "\n",
    "x_f = torch.rand(N_f, 1)*np.pi\n",
    "t_f = torch.rand(N_f, 1)*t_end\n",
    "# u_f = u_sol(t_f,x_f,n =1)\n",
    "\n",
    "#Defining the data points\n",
    "x_data = torch.tensor([[np.pi/2]])\n",
    "t_data = torch.tensor([[0.0]])\n",
    "u_data = torch.tensor([[0.0]])\n",
    "\n",
    "data = {\n",
    "    \"x_bc\": x_bc, \"t_bc\": t_bc, \"u_bc\": u_bc,\n",
    "    \"x_ic\": x_ic, \"t_ic\": t_ic, \"u_ic\": u_ic, \"u_t_ic\": u_t_ic,\n",
    "    \"x_f\": x_f, \"t_f\": t_f,\n",
    "    \"x_data\": x_data, \"t_data\": t_data, \"u_data\": u_data\n",
    "}\n",
    "\n",
    "layers = [2,40,40,40,40,40,40,40,1]\n",
    "model = PINN(data, layers).to(device)\n",
    "\n",
    "\n",
    "x = torch.tensor([[np.pi/2]])\n",
    "t = torch.tensor([[0.0]])\n",
    "u_true = torch.tensor([[0.0]])\n",
    "\n",
    "\n",
    "#Gathering everything needed for model setup in data\n",
    "\n",
    "\n",
    "\n",
    "# model.compute_total_loss(x,t,torch.tensor([[0.0]],device=device))\n",
    "# model.compute_boundary_loss()\n",
    "model.train_model(60000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1fe03ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24c23497290>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_val = 1\n",
    "\n",
    "X_test = torch.linspace(0, np.pi*2, 100).reshape(-1, 1)\n",
    "T_test = torch.ones_like(X_test)*t_val\n",
    "\n",
    "u_pred = model.predict(X_test, T_test)\n",
    "u_true = u_sol(t_val, X_test.numpy(), n=1)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "ax.plot(X_test, u_pred, label='Predicted', color='blue')\n",
    "ax.plot(X_test, u_true, label='Exact', color='red', linestyle='dashed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f0151b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_bc torch.Size([600, 1])\n",
      "t_bc torch.Size([600, 1])\n",
      "u_bc torch.Size([600, 1])\n",
      "x_ic torch.Size([300, 1])\n",
      "t_ic torch.Size([300, 1])\n",
      "u_ic torch.Size([300, 1])\n",
      "u_t_ic torch.Size([300, 1])\n",
      "x_f torch.Size([30000, 1])\n",
      "t_f torch.Size([30000, 1])\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"x_bc\": x_bc, \"t_bc\": t_bc, \"u_bc\": u_bc,\n",
    "    \"x_ic\": x_ic, \"t_ic\": t_ic, \"u_ic\": u_ic, \"u_t_ic\": u_t_ic,\n",
    "    \"x_f\": x_f, \"t_f\": t_f\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "798a6d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 0, Loss 1.751e+00, Time 0.38\n",
      "It 10, Loss 5.402e-01, Time 0.73\n",
      "It 20, Loss 4.634e-01, Time 0.71\n",
      "It 30, Loss 4.550e-01, Time 0.71\n",
      "It 40, Loss 4.438e-01, Time 0.71\n",
      "It 50, Loss 4.382e-01, Time 0.72\n",
      "It 60, Loss 4.349e-01, Time 0.72\n",
      "It 70, Loss 4.304e-01, Time 0.71\n",
      "It 80, Loss 4.254e-01, Time 0.72\n",
      "It 90, Loss 4.188e-01, Time 0.71\n",
      "It 100, Loss 4.103e-01, Time 0.72\n",
      "It 110, Loss 3.994e-01, Time 0.72\n",
      "It 120, Loss 3.850e-01, Time 0.71\n",
      "It 130, Loss 3.637e-01, Time 0.71\n",
      "It 140, Loss 3.297e-01, Time 0.72\n",
      "It 150, Loss 2.810e-01, Time 0.72\n",
      "It 160, Loss 2.280e-01, Time 0.72\n",
      "It 170, Loss 1.853e-01, Time 0.72\n",
      "It 180, Loss 1.624e-01, Time 0.71\n",
      "It 190, Loss 1.504e-01, Time 0.72\n",
      "It 200, Loss 1.397e-01, Time 0.71\n",
      "It 210, Loss 1.298e-01, Time 0.71\n",
      "It 220, Loss 1.209e-01, Time 0.71\n",
      "It 230, Loss 1.126e-01, Time 0.72\n",
      "It 240, Loss 1.056e-01, Time 0.71\n",
      "It 250, Loss 1.019e-01, Time 0.72\n",
      "It 260, Loss 9.638e-02, Time 0.72\n",
      "It 270, Loss 9.093e-02, Time 0.71\n",
      "It 280, Loss 8.730e-02, Time 0.71\n",
      "It 290, Loss 8.412e-02, Time 0.72\n",
      "It 300, Loss 8.187e-02, Time 0.71\n",
      "It 310, Loss 8.279e-02, Time 0.71\n",
      "It 320, Loss 7.894e-02, Time 0.72\n",
      "It 330, Loss 7.613e-02, Time 0.71\n",
      "It 340, Loss 7.430e-02, Time 0.71\n",
      "It 350, Loss 7.269e-02, Time 0.72\n",
      "It 360, Loss 7.131e-02, Time 0.71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 188\u001b[39m\n\u001b[32m    185\u001b[39m model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub, device)\n\u001b[32m    187\u001b[39m start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining time: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[33m\"\u001b[39m % (time.time() - start_time))\n\u001b[32m    191\u001b[39m u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mPhysicsInformedNN.train_model\u001b[39m\u001b[34m(self, nIter, lr)\u001b[39m\n\u001b[32m    108\u001b[39m optimizer.zero_grad()\n\u001b[32m    109\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.loss_fn()\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m optimizer.step()\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m it % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakob\\anaconda3\\envs\\Master\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakob\\anaconda3\\envs\\Master\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakob\\anaconda3\\envs\\Master\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import griddata\n",
    "from pyDOE import lhs\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "class PhysicsInformedNN(nn.Module):\n",
    "    def __init__(self, x0, u0, v0, tb, X_f, layers, lb, ub, device=\"cpu\"):\n",
    "        super(PhysicsInformedNN, self).__init__()\n",
    "\n",
    "        self.lb = torch.tensor(lb, dtype=torch.float32, device=device)\n",
    "        self.ub = torch.tensor(ub, dtype=torch.float32, device=device)\n",
    "        self.device = device\n",
    "\n",
    "        # Training data\n",
    "        self.x0 = torch.tensor(x0, dtype=torch.float32, device=device)\n",
    "        self.t0 = torch.tensor(np.zeros_like(x0), dtype=torch.float32, device=device)\n",
    "        self.u0 = torch.tensor(u0, dtype=torch.float32, device=device)\n",
    "        self.v0 = torch.tensor(v0, dtype=torch.float32, device=device)\n",
    "\n",
    "        self.x_lb = torch.tensor(np.zeros_like(tb) + lb[0], dtype=torch.float32, device=device)\n",
    "        self.t_lb = torch.tensor(tb, dtype=torch.float32, device=device)\n",
    "        self.x_ub = torch.tensor(np.zeros_like(tb) + ub[0], dtype=torch.float32, device=device)\n",
    "        self.t_ub = torch.tensor(tb, dtype=torch.float32, device=device)\n",
    "\n",
    "        self.x_f = torch.tensor(X_f[:, 0:1], dtype=torch.float32, device=device)\n",
    "        self.t_f = torch.tensor(X_f[:, 1:2], dtype=torch.float32, device=device)\n",
    "\n",
    "        # Define neural network\n",
    "        self.layers = layers\n",
    "        self.net = self.build_network(layers).to(device)\n",
    "\n",
    "    def build_network(self, layers):\n",
    "        \"\"\"Fully-connected MLP with tanh activations.\"\"\"\n",
    "        modules = []\n",
    "        for i in range(len(layers) - 2):\n",
    "            modules.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "            nn.init.xavier_normal_(modules[-1].weight)\n",
    "            modules.append(nn.Tanh())\n",
    "        modules.append(nn.Linear(layers[-2], layers[-1]))\n",
    "        nn.init.xavier_normal_(modules[-1].weight)\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        X = torch.cat([x, t], dim=1)\n",
    "        # scale input to [-1, 1]\n",
    "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0\n",
    "        uv = self.net(H)\n",
    "        u = uv[:, 0:1]\n",
    "        v = uv[:, 1:2]\n",
    "        return u, v\n",
    "\n",
    "    def net_uv(self, x, t):\n",
    "        \"\"\"Compute u, v and their spatial derivatives.\"\"\"\n",
    "        x.requires_grad_(True)\n",
    "        t.requires_grad_(True)\n",
    "        u, v = self.forward(x, t)\n",
    "\n",
    "        u_x = autograd.grad(u, x, torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "        v_x = autograd.grad(v, x, torch.ones_like(v), create_graph=True, retain_graph=True)[0]\n",
    "        return u, v, u_x, v_x\n",
    "\n",
    "    def net_f_uv(self, x, t):\n",
    "        \"\"\"Compute PDE residuals f_u and f_v.\"\"\"\n",
    "        u, v, u_x, v_x = self.net_uv(x, t)\n",
    "        u_t = autograd.grad(u, t, torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "        v_t = autograd.grad(v, t, torch.ones_like(v), create_graph=True, retain_graph=True)[0]\n",
    "        u_xx = autograd.grad(u_x, x, torch.ones_like(u_x), create_graph=True, retain_graph=True)[0]\n",
    "        v_xx = autograd.grad(v_x, x, torch.ones_like(v_x), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "        f_u = u_t + 0.5 * v_xx + (u ** 2 + v ** 2) * v\n",
    "        f_v = v_t - 0.5 * u_xx - (u ** 2 + v ** 2) * u\n",
    "        return f_u, f_v\n",
    "\n",
    "    def loss_fn(self):\n",
    "        \"\"\"Physics-informed loss combining data, boundary, and PDE terms.\"\"\"\n",
    "        # Initial condition\n",
    "        u0_pred, v0_pred = self.forward(self.x0, self.t0)\n",
    "        mse_u0 = torch.mean((self.u0 - u0_pred) ** 2)\n",
    "        mse_v0 = torch.mean((self.v0 - v0_pred) ** 2)\n",
    "\n",
    "        # Boundary condition\n",
    "        u_lb, v_lb, u_x_lb, v_x_lb = self.net_uv(self.x_lb, self.t_lb)\n",
    "        u_ub, v_ub, u_x_ub, v_x_ub = self.net_uv(self.x_ub, self.t_ub)\n",
    "        mse_bc = torch.mean((u_lb - u_ub) ** 2) + torch.mean((v_lb - v_ub) ** 2)\n",
    "        mse_bc += torch.mean((u_x_lb - u_x_ub) ** 2) + torch.mean((v_x_lb - v_x_ub) ** 2)\n",
    "\n",
    "        # PDE residual\n",
    "        f_u, f_v = self.net_f_uv(self.x_f, self.t_f)\n",
    "        mse_f = torch.mean(f_u ** 2) + torch.mean(f_v ** 2)\n",
    "\n",
    "        loss = mse_u0 + mse_v0 + mse_bc + mse_f\n",
    "        return loss\n",
    "\n",
    "    def train_model(self, nIter, lr=1e-3):\n",
    "        \"\"\"Train using Adam, then switch to L-BFGS.\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.loss_fn()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"It {it}, Loss {loss.item():.3e}, Time {elapsed:.2f}\")\n",
    "                start_time = time.time()\n",
    "\n",
    "        # Switch to L-BFGS\n",
    "        def closure():\n",
    "            optimizer_LBFGS.zero_grad()\n",
    "            loss = self.loss_fn()\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer_LBFGS = torch.optim.LBFGS(\n",
    "            self.net.parameters(),\n",
    "            max_iter=50000,\n",
    "            tolerance_grad=1e-12,\n",
    "            tolerance_change=1e-12,\n",
    "            history_size=50,\n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        print(\"Starting L-BFGS optimization...\")\n",
    "        optimizer_LBFGS.step(closure)\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        x_star = torch.tensor(X_star[:, 0:1], dtype=torch.float32, device=self.device)\n",
    "        t_star = torch.tensor(X_star[:, 1:2], dtype=torch.float32, device=self.device)\n",
    "        u_star, v_star = self.forward(x_star, t_star)\n",
    "        f_u, f_v = self.net_f_uv(x_star, t_star)\n",
    "        return (\n",
    "            u_star.detach().cpu().numpy(),\n",
    "            v_star.detach().cpu().numpy(),\n",
    "            f_u.detach().cpu().numpy(),\n",
    "            f_v.detach().cpu().numpy(),\n",
    "        )\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Example usage (like original)\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    noise = 0.0\n",
    "    lb = np.array([-5.0, 0.0])\n",
    "    ub = np.array([5.0, np.pi / 2])\n",
    "    N0, N_b, N_f = 50, 50, 20000\n",
    "    layers = [2, 100, 100, 100, 100, 2]\n",
    "\n",
    "    data = loadmat(\"C:\\\\Users\\\\jakob\\\\Desktop\\\\Master thesis\\\\Gihub Code\\\\Master\\\\Notebooks\\\\Data\\\\NLS.mat\")\n",
    "    t = data[\"tt\"].flatten()[:, None]\n",
    "    x = data[\"x\"].flatten()[:, None]\n",
    "    Exact = data[\"uu\"]\n",
    "    Exact_u = np.real(Exact)\n",
    "    Exact_v = np.imag(Exact)\n",
    "\n",
    "    X, T = np.meshgrid(x, t)\n",
    "    X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "    u_star = Exact_u.T.flatten()[:, None]\n",
    "    v_star = Exact_v.T.flatten()[:, None]\n",
    "    h_star = np.sqrt(u_star**2 + v_star**2)\n",
    "\n",
    "    # Initial & boundary data\n",
    "    idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "    x0 = x[idx_x, :]\n",
    "    u0 = Exact_u[idx_x, 0:1]\n",
    "    v0 = Exact_v[idx_x, 0:1]\n",
    "\n",
    "    idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "    tb = t[idx_t, :]\n",
    "\n",
    "    X_f = lb + (ub - lb) * lhs(2, N_f)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub, device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.train_model(50000)\n",
    "    print(\"Training time: %.4f\" % (time.time() - start_time))\n",
    "\n",
    "    u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star)\n",
    "    h_pred = np.sqrt(u_pred ** 2 + v_pred ** 2)\n",
    "\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_v = np.linalg.norm(v_star - v_pred, 2) / np.linalg.norm(v_star, 2)\n",
    "    error_h = np.linalg.norm(h_star - h_pred, 2) / np.linalg.norm(h_star, 2)\n",
    "    print(f\"Error u: {error_u:e}\")\n",
    "    print(f\"Error v: {error_v:e}\")\n",
    "    print(f\"Error h: {error_h:e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b5f2059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    10 | L_pde 4.376e-05 | L_bc 3.826e-02 | L_ic 3.067e-01 | total 3.450e-01\n",
      "step    20 | L_pde 1.886e-04 | L_bc 8.827e-02 | L_ic 2.396e-01 | total 3.280e-01\n",
      "step    30 | L_pde 1.880e-04 | L_bc 7.512e-02 | L_ic 2.492e-01 | total 3.245e-01\n",
      "step    40 | L_pde 1.359e-03 | L_bc 6.622e-02 | L_ic 2.518e-01 | total 3.193e-01\n",
      "step    50 | L_pde 5.356e-04 | L_bc 7.694e-02 | L_ic 2.282e-01 | total 3.057e-01\n",
      "step    60 | L_pde 1.832e-03 | L_bc 7.356e-02 | L_ic 2.125e-01 | total 2.879e-01\n",
      "step    70 | L_pde 1.186e-03 | L_bc 6.405e-02 | L_ic 2.155e-01 | total 2.807e-01\n",
      "step    80 | L_pde 3.735e-03 | L_bc 6.722e-02 | L_ic 1.931e-01 | total 2.640e-01\n",
      "step    90 | L_pde 2.314e-02 | L_bc 6.665e-02 | L_ic 1.504e-01 | total 2.401e-01\n",
      "step   100 | L_pde 1.711e-02 | L_bc 5.059e-02 | L_ic 1.388e-01 | total 2.065e-01\n",
      "step   110 | L_pde 1.275e-02 | L_bc 5.673e-02 | L_ic 8.721e-02 | total 1.567e-01\n",
      "step   120 | L_pde 1.457e-02 | L_bc 4.695e-02 | L_ic 5.338e-02 | total 1.149e-01\n",
      "step   130 | L_pde 1.409e-02 | L_bc 5.009e-02 | L_ic 5.724e-02 | total 1.214e-01\n",
      "step   140 | L_pde 1.373e-02 | L_bc 3.445e-02 | L_ic 3.894e-02 | total 8.712e-02\n",
      "step   150 | L_pde 6.908e-03 | L_bc 4.571e-02 | L_ic 2.374e-02 | total 7.636e-02\n",
      "step   160 | L_pde 5.870e-03 | L_bc 3.385e-02 | L_ic 2.346e-02 | total 6.319e-02\n",
      "step   170 | L_pde 4.714e-03 | L_bc 3.651e-02 | L_ic 1.446e-02 | total 5.569e-02\n",
      "step   180 | L_pde 3.674e-03 | L_bc 3.087e-02 | L_ic 1.604e-02 | total 5.059e-02\n",
      "step   190 | L_pde 2.888e-02 | L_bc 2.588e-02 | L_ic 2.763e-02 | total 8.239e-02\n",
      "step   200 | L_pde 6.210e-03 | L_bc 3.040e-02 | L_ic 1.924e-02 | total 5.585e-02\n",
      "step   210 | L_pde 2.768e-03 | L_bc 2.783e-02 | L_ic 1.693e-02 | total 4.753e-02\n",
      "step   220 | L_pde 3.939e-03 | L_bc 2.582e-02 | L_ic 1.567e-02 | total 4.543e-02\n",
      "step   230 | L_pde 3.519e-03 | L_bc 2.465e-02 | L_ic 1.562e-02 | total 4.378e-02\n",
      "step   240 | L_pde 1.698e-03 | L_bc 2.534e-02 | L_ic 1.515e-02 | total 4.219e-02\n",
      "step   250 | L_pde 1.737e-03 | L_bc 2.413e-02 | L_ic 1.498e-02 | total 4.086e-02\n",
      "step   260 | L_pde 1.500e-03 | L_bc 2.355e-02 | L_ic 1.511e-02 | total 4.016e-02\n",
      "step   270 | L_pde 1.437e-03 | L_bc 2.331e-02 | L_ic 1.470e-02 | total 3.945e-02\n",
      "step   280 | L_pde 1.180e-03 | L_bc 2.306e-02 | L_ic 1.456e-02 | total 3.880e-02\n",
      "step   290 | L_pde 1.077e-03 | L_bc 2.283e-02 | L_ic 1.431e-02 | total 3.821e-02\n",
      "step   300 | L_pde 1.042e-03 | L_bc 2.248e-02 | L_ic 1.411e-02 | total 3.763e-02\n",
      "step   310 | L_pde 1.001e-03 | L_bc 2.214e-02 | L_ic 1.393e-02 | total 3.707e-02\n",
      "step   320 | L_pde 1.807e-03 | L_bc 2.127e-02 | L_ic 1.445e-02 | total 3.752e-02\n",
      "step   330 | L_pde 2.202e-02 | L_bc 3.240e-02 | L_ic 3.170e-02 | total 8.612e-02\n",
      "step   340 | L_pde 5.324e-04 | L_bc 2.295e-02 | L_ic 1.528e-02 | total 3.876e-02\n",
      "step   350 | L_pde 1.086e-03 | L_bc 2.193e-02 | L_ic 1.703e-02 | total 4.004e-02\n",
      "step   360 | L_pde 1.103e-03 | L_bc 2.119e-02 | L_ic 1.429e-02 | total 3.659e-02\n",
      "step   370 | L_pde 1.086e-03 | L_bc 2.096e-02 | L_ic 1.332e-02 | total 3.536e-02\n",
      "step   380 | L_pde 1.108e-03 | L_bc 2.077e-02 | L_ic 1.268e-02 | total 3.455e-02\n",
      "step   390 | L_pde 8.809e-04 | L_bc 2.011e-02 | L_ic 1.297e-02 | total 3.396e-02\n",
      "step   400 | L_pde 7.922e-04 | L_bc 2.018e-02 | L_ic 1.249e-02 | total 3.346e-02\n",
      "step   410 | L_pde 8.536e-04 | L_bc 1.970e-02 | L_ic 1.243e-02 | total 3.298e-02\n",
      "step   420 | L_pde 7.903e-04 | L_bc 1.955e-02 | L_ic 1.216e-02 | total 3.249e-02\n",
      "step   430 | L_pde 7.589e-04 | L_bc 1.931e-02 | L_ic 1.195e-02 | total 3.201e-02\n",
      "step   440 | L_pde 7.509e-04 | L_bc 1.899e-02 | L_ic 1.178e-02 | total 3.152e-02\n",
      "step   450 | L_pde 7.453e-04 | L_bc 1.869e-02 | L_ic 1.158e-02 | total 3.102e-02\n",
      "step   460 | L_pde 7.337e-04 | L_bc 1.840e-02 | L_ic 1.139e-02 | total 3.052e-02\n",
      "step   470 | L_pde 1.268e-03 | L_bc 2.144e-02 | L_ic 2.076e-02 | total 4.347e-02\n",
      "step   480 | L_pde 9.657e-03 | L_bc 2.358e-02 | L_ic 2.482e-02 | total 5.806e-02\n",
      "step   490 | L_pde 7.069e-03 | L_bc 2.448e-02 | L_ic 1.032e-02 | total 4.186e-02\n",
      "step   500 | L_pde 5.782e-04 | L_bc 2.089e-02 | L_ic 1.176e-02 | total 3.324e-02\n",
      "\n",
      "Relative L2 error on grid: 2.655e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakob\\anaconda3\\envs\\thesis\\Lib\\site-packages\\torch\\functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4324.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# wave_pinn_fixed.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "import math\n",
    "\n",
    "# ----- Setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# ----- Derivative helper\n",
    "def d(outputs, inputs, create_graph=True):\n",
    "    \"\"\"First derivative: ∂outputs/∂inputs via vector–Jacobian product with ones.\"\"\"\n",
    "    return grad(outputs, inputs, torch.ones_like(outputs), create_graph=create_graph)[0]\n",
    "\n",
    "# ----- Network\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, in_dim=2, width=128, depth=4, act=nn.Tanh()):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            layers += [nn.Linear(in_dim if i == 0 else width, width), act]\n",
    "        layers += [nn.Linear(width, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        z = torch.cat([x, t], dim=1)  # (N,2)\n",
    "        return self.net(z)\n",
    "\n",
    "# ----- Problem setup\n",
    "c = 1.0\n",
    "N_f = 10_000   # collocation points\n",
    "N_b = 1_000    # boundary points\n",
    "N_i = 1_000    # initial line points\n",
    "\n",
    "rand = lambda n: torch.rand(n, 1, device=device)\n",
    "\n",
    "# Interior points (we differentiate w.r.t. these each step)\n",
    "x_f = rand(N_f)\n",
    "t_f = rand(N_f)\n",
    "\n",
    "# Boundary points (no derivatives needed)\n",
    "t_b = rand(N_b)\n",
    "x_b0 = torch.zeros_like(t_b, device=device)\n",
    "x_b1 = torch.ones_like(t_b, device=device)\n",
    "\n",
    "# Initial line t=0 (we differentiate wrt t here)\n",
    "x_i = rand(N_i)\n",
    "t_i0 = torch.zeros_like(x_i, device=device)\n",
    "\n",
    "# Ground-truth functions (for IC & evaluation)\n",
    "def u_true(x, t):\n",
    "    return torch.sin(math.pi * x) * torch.cos(math.pi * t)\n",
    "\n",
    "def ut_true(x, t):\n",
    "    return -math.pi * torch.sin(math.pi * x) * torch.sin(math.pi * t)\n",
    "\n",
    "# IC targets (detach to avoid any graph ties)\n",
    "with torch.no_grad():\n",
    "    u0 = u_true(x_i, torch.zeros_like(x_i))\n",
    "    v0 = ut_true(x_i, torch.zeros_like(x_i))  # equals 0 for this choice\n",
    "\n",
    "# ----- Model & optimizer\n",
    "model = PINN().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "mse = lambda x: torch.mean(x**2)\n",
    "\n",
    "# ----- PDE residual\n",
    "def pde_residual(net, x, t):\n",
    "    u    = net(x, t)       # (N,1)\n",
    "    u_t  = d(u, t)         # ∂u/∂t\n",
    "    u_tt = d(u_t, t)       # ∂²u/∂t²\n",
    "    u_x  = d(u, x)         # ∂u/∂x\n",
    "    u_xx = d(u_x, x)       # ∂²u/∂x²\n",
    "    return u_tt - (c**2) * u_xx\n",
    "\n",
    "# ----- Training\n",
    "steps = 500\n",
    "for step in range(1, steps + 1):\n",
    "    # Fresh leaves for tensors we differentiate w.r.t. this step\n",
    "    x_f = x_f.detach().requires_grad_(True)\n",
    "    t_f = t_f.detach().requires_grad_(True)\n",
    "    x_i = x_i.detach().requires_grad_(True)\n",
    "    t_i0 = t_i0.detach().requires_grad_(True)\n",
    "\n",
    "    # PDE loss\n",
    "    r = pde_residual(model, x_f, t_f)\n",
    "    L_pde = mse(r)\n",
    "\n",
    "    # Boundary (Dirichlet u=0 at x=0 and x=1) — no grads w.r.t. coords needed\n",
    "    u_b0 = model(x_b0, t_b)\n",
    "    u_b1 = model(x_b1, t_b)\n",
    "    L_bc = mse(u_b0) + mse(u_b1)\n",
    "\n",
    "    # Initial conditions at t=0: u(x,0)=sin(pi x), u_t(x,0)=0\n",
    "    u_i  = model(x_i, t_i0)\n",
    "    u_ti = d(u_i, t_i0)  # ∂u/∂t at t=0\n",
    "    # u0, v0 are constants (no grad)\n",
    "    L_ic = mse(u_i - u0) + mse(u_ti - v0)\n",
    "\n",
    "    loss = L_pde + L_bc + L_ic\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step:5d} | L_pde {L_pde.item():.3e} | L_bc {L_bc.item():.3e} | L_ic {L_ic.item():.3e} | total {loss.item():.3e}\")\n",
    "\n",
    "# ----- Evaluation\n",
    "with torch.no_grad():\n",
    "    nx, nt = 200, 200\n",
    "    xg = torch.linspace(0, 1, nx, device=device).view(-1, 1)\n",
    "    tg = torch.linspace(0, 1, nt, device=device).view(-1, 1)\n",
    "    # default indexing in torch.meshgrid is fine here\n",
    "    Xg, Tg = torch.meshgrid(xg.squeeze(1), tg.squeeze(1))\n",
    "    x_flat = Xg.reshape(-1, 1)\n",
    "    t_flat = Tg.reshape(-1, 1)\n",
    "\n",
    "    up = model(x_flat, t_flat).reshape(nx, nt)\n",
    "    ut = u_true(Xg, Tg)\n",
    "\n",
    "    num = torch.linalg.norm((up - ut).reshape(-1))\n",
    "    den = torch.linalg.norm(ut.reshape(-1))\n",
    "    rel_l2 = (num / den).item()\n",
    "    print(f\"\\nRelative L2 error on grid: {rel_l2:.3e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
