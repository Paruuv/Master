{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68793758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1221]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "def network_dt(outputs, inputs, create_graph = True):\n",
    "    return grad(outputs, inputs, torch.ones_like(outputs), create_graph=create_graph)[0]\n",
    "\n",
    "def network_ddt(outputs, inputs, create_graph = True):\n",
    "\n",
    "    dt = network_dt(outputs,inputs, create_graph=create_graph)\n",
    "    return grad(dt, inputs, torch.ones_like(dt),create_graph=create_graph)[0]\n",
    "\n",
    "def sample_uniform(n): \n",
    "    return torch.rand(n, 1, device=device)\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, width=128, depth=3, act=nn.Tanh()):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            layers += [nn.Linear(2 if i==0 else width, width), act]\n",
    "        layers += [nn.Linear(width, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
    "    def forward(self, x, t):\n",
    "        return self.net(torch.cat([x, t], dim=1))\n",
    "\n",
    "model = PINN().to(device)\n",
    "\n",
    "def compute_PDE_loss(net,x,t,N,L):\n",
    "    dx = L/N\n",
    "    y = net(x,t)\n",
    "    yddx = (net(x+dx,t)-2*y+net(x-dx,t))/dx**2\n",
    "    ydt = grad(y,t,torch.ones_like(y),retain_graph=True,create_graph=True)[0]\n",
    "    yddt = grad(ydt,t,torch.ones_like(y),retain_graph=True,create_graph=True)[0]\n",
    "    residual = yddt - yddx\n",
    "    return residual\n",
    "\n",
    "L = np.pi\n",
    "N = 100\n",
    "\n",
    "x = torch.tensor([[1.0]], device=device)\n",
    "t = torch.tensor([[0.25]], device=device,requires_grad=True)\n",
    "\n",
    "compute_PDE_loss(model,x,t,N,L)\n",
    "# test = model.forward(x,t)\n",
    "\n",
    "\n",
    "# # choose counts\n",
    "# N_u, N_f = 10, 100\n",
    "\n",
    "# # boundary/initial (examples; targets added in Step 4)\n",
    "# x_b0 = torch.zeros(N_u//2, 1, device=device); t_b0 = sample_uniform(N_u//2)  # x=0\n",
    "# x_b1 = torch.ones (N_u//2, 1, device=device); t_b1 = sample_uniform(N_u//2)  # x=1\n",
    "# x_i  = sample_uniform(N_u);                 t_i0 = torch.zeros_like(x_i)      # t=0\n",
    "\n",
    "# # interior collocation\n",
    "# x_f = sample_uniform(N_f); t_f = sample_uniform(N_f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b5f2059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    10 | L_pde 4.376e-05 | L_bc 3.826e-02 | L_ic 3.067e-01 | total 3.450e-01\n",
      "step    20 | L_pde 1.886e-04 | L_bc 8.827e-02 | L_ic 2.396e-01 | total 3.280e-01\n",
      "step    30 | L_pde 1.880e-04 | L_bc 7.512e-02 | L_ic 2.492e-01 | total 3.245e-01\n",
      "step    40 | L_pde 1.359e-03 | L_bc 6.622e-02 | L_ic 2.518e-01 | total 3.193e-01\n",
      "step    50 | L_pde 5.356e-04 | L_bc 7.694e-02 | L_ic 2.282e-01 | total 3.057e-01\n",
      "step    60 | L_pde 1.832e-03 | L_bc 7.356e-02 | L_ic 2.125e-01 | total 2.879e-01\n",
      "step    70 | L_pde 1.186e-03 | L_bc 6.405e-02 | L_ic 2.155e-01 | total 2.807e-01\n",
      "step    80 | L_pde 3.735e-03 | L_bc 6.722e-02 | L_ic 1.931e-01 | total 2.640e-01\n",
      "step    90 | L_pde 2.314e-02 | L_bc 6.665e-02 | L_ic 1.504e-01 | total 2.401e-01\n",
      "step   100 | L_pde 1.711e-02 | L_bc 5.059e-02 | L_ic 1.388e-01 | total 2.065e-01\n",
      "step   110 | L_pde 1.275e-02 | L_bc 5.673e-02 | L_ic 8.721e-02 | total 1.567e-01\n",
      "step   120 | L_pde 1.457e-02 | L_bc 4.695e-02 | L_ic 5.338e-02 | total 1.149e-01\n",
      "step   130 | L_pde 1.409e-02 | L_bc 5.009e-02 | L_ic 5.724e-02 | total 1.214e-01\n",
      "step   140 | L_pde 1.373e-02 | L_bc 3.445e-02 | L_ic 3.894e-02 | total 8.712e-02\n",
      "step   150 | L_pde 6.908e-03 | L_bc 4.571e-02 | L_ic 2.374e-02 | total 7.636e-02\n",
      "step   160 | L_pde 5.870e-03 | L_bc 3.385e-02 | L_ic 2.346e-02 | total 6.319e-02\n",
      "step   170 | L_pde 4.714e-03 | L_bc 3.651e-02 | L_ic 1.446e-02 | total 5.569e-02\n",
      "step   180 | L_pde 3.674e-03 | L_bc 3.087e-02 | L_ic 1.604e-02 | total 5.059e-02\n",
      "step   190 | L_pde 2.888e-02 | L_bc 2.588e-02 | L_ic 2.763e-02 | total 8.239e-02\n",
      "step   200 | L_pde 6.210e-03 | L_bc 3.040e-02 | L_ic 1.924e-02 | total 5.585e-02\n",
      "step   210 | L_pde 2.768e-03 | L_bc 2.783e-02 | L_ic 1.693e-02 | total 4.753e-02\n",
      "step   220 | L_pde 3.939e-03 | L_bc 2.582e-02 | L_ic 1.567e-02 | total 4.543e-02\n",
      "step   230 | L_pde 3.519e-03 | L_bc 2.465e-02 | L_ic 1.562e-02 | total 4.378e-02\n",
      "step   240 | L_pde 1.698e-03 | L_bc 2.534e-02 | L_ic 1.515e-02 | total 4.219e-02\n",
      "step   250 | L_pde 1.737e-03 | L_bc 2.413e-02 | L_ic 1.498e-02 | total 4.086e-02\n",
      "step   260 | L_pde 1.500e-03 | L_bc 2.355e-02 | L_ic 1.511e-02 | total 4.016e-02\n",
      "step   270 | L_pde 1.437e-03 | L_bc 2.331e-02 | L_ic 1.470e-02 | total 3.945e-02\n",
      "step   280 | L_pde 1.180e-03 | L_bc 2.306e-02 | L_ic 1.456e-02 | total 3.880e-02\n",
      "step   290 | L_pde 1.077e-03 | L_bc 2.283e-02 | L_ic 1.431e-02 | total 3.821e-02\n",
      "step   300 | L_pde 1.042e-03 | L_bc 2.248e-02 | L_ic 1.411e-02 | total 3.763e-02\n",
      "step   310 | L_pde 1.001e-03 | L_bc 2.214e-02 | L_ic 1.393e-02 | total 3.707e-02\n",
      "step   320 | L_pde 1.807e-03 | L_bc 2.127e-02 | L_ic 1.445e-02 | total 3.752e-02\n",
      "step   330 | L_pde 2.202e-02 | L_bc 3.240e-02 | L_ic 3.170e-02 | total 8.612e-02\n",
      "step   340 | L_pde 5.324e-04 | L_bc 2.295e-02 | L_ic 1.528e-02 | total 3.876e-02\n",
      "step   350 | L_pde 1.086e-03 | L_bc 2.193e-02 | L_ic 1.703e-02 | total 4.004e-02\n",
      "step   360 | L_pde 1.103e-03 | L_bc 2.119e-02 | L_ic 1.429e-02 | total 3.659e-02\n",
      "step   370 | L_pde 1.086e-03 | L_bc 2.096e-02 | L_ic 1.332e-02 | total 3.536e-02\n",
      "step   380 | L_pde 1.108e-03 | L_bc 2.077e-02 | L_ic 1.268e-02 | total 3.455e-02\n",
      "step   390 | L_pde 8.809e-04 | L_bc 2.011e-02 | L_ic 1.297e-02 | total 3.396e-02\n",
      "step   400 | L_pde 7.922e-04 | L_bc 2.018e-02 | L_ic 1.249e-02 | total 3.346e-02\n",
      "step   410 | L_pde 8.536e-04 | L_bc 1.970e-02 | L_ic 1.243e-02 | total 3.298e-02\n",
      "step   420 | L_pde 7.903e-04 | L_bc 1.955e-02 | L_ic 1.216e-02 | total 3.249e-02\n",
      "step   430 | L_pde 7.589e-04 | L_bc 1.931e-02 | L_ic 1.195e-02 | total 3.201e-02\n",
      "step   440 | L_pde 7.509e-04 | L_bc 1.899e-02 | L_ic 1.178e-02 | total 3.152e-02\n",
      "step   450 | L_pde 7.453e-04 | L_bc 1.869e-02 | L_ic 1.158e-02 | total 3.102e-02\n",
      "step   460 | L_pde 7.337e-04 | L_bc 1.840e-02 | L_ic 1.139e-02 | total 3.052e-02\n",
      "step   470 | L_pde 1.268e-03 | L_bc 2.144e-02 | L_ic 2.076e-02 | total 4.347e-02\n",
      "step   480 | L_pde 9.657e-03 | L_bc 2.358e-02 | L_ic 2.482e-02 | total 5.806e-02\n",
      "step   490 | L_pde 7.069e-03 | L_bc 2.448e-02 | L_ic 1.032e-02 | total 4.186e-02\n",
      "step   500 | L_pde 5.782e-04 | L_bc 2.089e-02 | L_ic 1.176e-02 | total 3.324e-02\n",
      "\n",
      "Relative L2 error on grid: 2.655e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakob\\anaconda3\\envs\\thesis\\Lib\\site-packages\\torch\\functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4324.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# wave_pinn_fixed.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "import math\n",
    "\n",
    "# ----- Setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# ----- Derivative helper\n",
    "def d(outputs, inputs, create_graph=True):\n",
    "    \"\"\"First derivative: ∂outputs/∂inputs via vector–Jacobian product with ones.\"\"\"\n",
    "    return grad(outputs, inputs, torch.ones_like(outputs), create_graph=create_graph)[0]\n",
    "\n",
    "# ----- Network\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, in_dim=2, width=128, depth=4, act=nn.Tanh()):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            layers += [nn.Linear(in_dim if i == 0 else width, width), act]\n",
    "        layers += [nn.Linear(width, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        z = torch.cat([x, t], dim=1)  # (N,2)\n",
    "        return self.net(z)\n",
    "\n",
    "# ----- Problem setup\n",
    "c = 1.0\n",
    "N_f = 10_000   # collocation points\n",
    "N_b = 1_000    # boundary points\n",
    "N_i = 1_000    # initial line points\n",
    "\n",
    "rand = lambda n: torch.rand(n, 1, device=device)\n",
    "\n",
    "# Interior points (we differentiate w.r.t. these each step)\n",
    "x_f = rand(N_f)\n",
    "t_f = rand(N_f)\n",
    "\n",
    "# Boundary points (no derivatives needed)\n",
    "t_b = rand(N_b)\n",
    "x_b0 = torch.zeros_like(t_b, device=device)\n",
    "x_b1 = torch.ones_like(t_b, device=device)\n",
    "\n",
    "# Initial line t=0 (we differentiate wrt t here)\n",
    "x_i = rand(N_i)\n",
    "t_i0 = torch.zeros_like(x_i, device=device)\n",
    "\n",
    "# Ground-truth functions (for IC & evaluation)\n",
    "def u_true(x, t):\n",
    "    return torch.sin(math.pi * x) * torch.cos(math.pi * t)\n",
    "\n",
    "def ut_true(x, t):\n",
    "    return -math.pi * torch.sin(math.pi * x) * torch.sin(math.pi * t)\n",
    "\n",
    "# IC targets (detach to avoid any graph ties)\n",
    "with torch.no_grad():\n",
    "    u0 = u_true(x_i, torch.zeros_like(x_i))\n",
    "    v0 = ut_true(x_i, torch.zeros_like(x_i))  # equals 0 for this choice\n",
    "\n",
    "# ----- Model & optimizer\n",
    "model = PINN().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "mse = lambda x: torch.mean(x**2)\n",
    "\n",
    "# ----- PDE residual\n",
    "def pde_residual(net, x, t):\n",
    "    u    = net(x, t)       # (N,1)\n",
    "    u_t  = d(u, t)         # ∂u/∂t\n",
    "    u_tt = d(u_t, t)       # ∂²u/∂t²\n",
    "    u_x  = d(u, x)         # ∂u/∂x\n",
    "    u_xx = d(u_x, x)       # ∂²u/∂x²\n",
    "    return u_tt - (c**2) * u_xx\n",
    "\n",
    "# ----- Training\n",
    "steps = 500\n",
    "for step in range(1, steps + 1):\n",
    "    # Fresh leaves for tensors we differentiate w.r.t. this step\n",
    "    x_f = x_f.detach().requires_grad_(True)\n",
    "    t_f = t_f.detach().requires_grad_(True)\n",
    "    x_i = x_i.detach().requires_grad_(True)\n",
    "    t_i0 = t_i0.detach().requires_grad_(True)\n",
    "\n",
    "    # PDE loss\n",
    "    r = pde_residual(model, x_f, t_f)\n",
    "    L_pde = mse(r)\n",
    "\n",
    "    # Boundary (Dirichlet u=0 at x=0 and x=1) — no grads w.r.t. coords needed\n",
    "    u_b0 = model(x_b0, t_b)\n",
    "    u_b1 = model(x_b1, t_b)\n",
    "    L_bc = mse(u_b0) + mse(u_b1)\n",
    "\n",
    "    # Initial conditions at t=0: u(x,0)=sin(pi x), u_t(x,0)=0\n",
    "    u_i  = model(x_i, t_i0)\n",
    "    u_ti = d(u_i, t_i0)  # ∂u/∂t at t=0\n",
    "    # u0, v0 are constants (no grad)\n",
    "    L_ic = mse(u_i - u0) + mse(u_ti - v0)\n",
    "\n",
    "    loss = L_pde + L_bc + L_ic\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step:5d} | L_pde {L_pde.item():.3e} | L_bc {L_bc.item():.3e} | L_ic {L_ic.item():.3e} | total {loss.item():.3e}\")\n",
    "\n",
    "# ----- Evaluation\n",
    "with torch.no_grad():\n",
    "    nx, nt = 200, 200\n",
    "    xg = torch.linspace(0, 1, nx, device=device).view(-1, 1)\n",
    "    tg = torch.linspace(0, 1, nt, device=device).view(-1, 1)\n",
    "    # default indexing in torch.meshgrid is fine here\n",
    "    Xg, Tg = torch.meshgrid(xg.squeeze(1), tg.squeeze(1))\n",
    "    x_flat = Xg.reshape(-1, 1)\n",
    "    t_flat = Tg.reshape(-1, 1)\n",
    "\n",
    "    up = model(x_flat, t_flat).reshape(nx, nt)\n",
    "    ut = u_true(Xg, Tg)\n",
    "\n",
    "    num = torch.linalg.norm((up - ut).reshape(-1))\n",
    "    den = torch.linalg.norm(ut.reshape(-1))\n",
    "    rel_l2 = (num / den).item()\n",
    "    print(f\"\\nRelative L2 error on grid: {rel_l2:.3e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
